
Title: AIML: Artificial Intelligence and Machine Learning 

Lecture Notes v1.16

Dr. Max A. Little

March 1, 2024

UNIVERSITY OF BIRMINGHAM

Contents
I. Introduction ..... 5
Section 1. Module overview
1.1. Reference material
1.2. How to use these lecture notes ..... 7
1.3. Course content ..... 7
1.4. Overview of AI and ML ..... 7
Section 2. Essential mathematical background ..... 9
2.1. Essential mathematical concepts ..... 9
2.2. Mathematical notation in this module ..... 9
2.3. Plotting functions ..... 9
II. Symbolic artificial intelligence ..... 11
Section 3. Combinatorial optimization ..... 12
3.1. Optimizing objective functions ..... 12
3.2. Exhaustive search and combinatorial explosion ..... 12
3.3. Complexity classes and worst case algorithm performance ..... 13
Section 4. Exact algorithms ..... 15
4.1. Algorithm strategies ..... 15
4.2. Sequential decision processes (SDPs) ..... 15
Section 5. Dynamic programming ..... 19
5.1. Efficient factorization and the principle of optimality ..... 19
5.2. Example: maximum sum tail subsequences ..... 20
Section 6. Approximate combinatorial algorithms ..... 23
6.1. Global and local optima ..... 23
6.2. Approximate greedy search ..... 24
6.3. Example: maximum sum combinations ..... 24
Section 7. Simulated annealing ..... 27
7.1. Escaping local optima ..... 27
7.2. Example: annealing better maximum sum combinations ..... 28
Section 8. Logic ..... 30
8.1. Propositional calculus ..... 30
8.2. Logical inference ..... 32
8.3. Example: automated medical decision-making ..... 32
III. Machine learning ..... 35
Section 9. Machine learning and gradient descent ..... 36
9.1. Overview of machine learning ..... 36
9.2. Training ML algorithms using sequential gradient descent (SGD) ..... 36
9.3. Example: linear regression ..... 37
9.4. Model complexity and generalization ..... 39
Section 10. Clustering ..... 41
10.1. Formulating optimal $K$-clustering ..... 41
10.2. The $K$-means algorithm ..... 42
10.3. Example: patch-based digital image dictionaries ..... 43
Section 11. Classification ..... 47
11.1. Partitioning feature space based on labeled training data ..... 47
11.2. The perfect classifier? ..... 47
11.3. Classification defined mathematically ..... 48
Section 12. The perceptron ..... 51
12.1. Linear classification with perceptron loss ..... 51
12.2. SGD for classification: the perceptron algorithm ..... 51
Section 13. Neural networks and deep learning ..... 56
13.1. Activation nonlinearities ..... 56
13.2. Deep neural networks: chained perceptrons ..... 56
13.3. Example: deep logic networks ..... 59
Section 14. Automatic differentiation ..... 62
14.1. Deep neural networks and automatic differentiation ..... 62
14.2. Chain rule of calculus and dual numbers ..... 63
14.3. Example: AD for multilayer perceptrons ..... 65
Section 15. Probability and probabilistic graphical models ..... 66
15.1. Rules of probability and probability distributions ..... 66
15.2. Two or more random variables together: marginals, conditionals, independence ..... 68
15.3. Probabilistic graphical models (PGMs) ..... 70
Section 16. Bayesian models ..... 72
16.1. Bayes' theorem ..... 72
16.2. Probabilistic classification using Bayes' theorem ..... 73
16.3. Example: identifying pulsars ..... 75
Section 17. Sequential problems and hidden Markov models ..... 78
17.1. Markov chains and the hidden Markov model ..... 78
17.2. Viterbi decoding ..... 79
17.3. Example: genome sequence region segmentation ..... 80
Section 18. Other sequential models ..... 84
18.1. Optimal sequential decision-making ..... 84
18.2. Kalman filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
18.3. Recurrent neural networks (RNN) . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88

Part I.

Introduction 

Section 1.

Module overview

Relevant references for this section are R\&N Section 1.1, MLSP Preface, and PRLM, Section 1 (up to start of Section 1.1).

1.1. Reference material

The main AI reference textbooks are (shorthand citation used in this module in bold):

$\triangleright$ Russell and Norvig, 2010, Artificial Intelligence: A Modern Approach, 3rd Edition/4th Edition, Prentice Hall $(\mathbf{R \& N})$

$\triangleright$ Cormen, Leiserson, Rivest and Stein, 2022, Introduction to Algorithms, 4th Edition, The MIT Press (CLRS)

The main ML reference textbooks are:

$\triangleright$ Bishop, 2006, Pattern Recognition and Machine Learning, Springer (PRML)

$\triangleright$ Little, 2019, Machine Learning for Signal Processing, Oxford University Press (MLSP)

$\triangleright$ Hastie and Tibshirani, 2008, The Elements of Statistical Learning, 2nd Edition, Springer (H\&T)

Background mathematics textbooks:

$\triangleright$ Gill, 2006, Essential Mathematics for Political and Social Research, Cambridge University Press (Gill)

$\triangleright$ Stirzaker, 2003, Elementary Probability, Cambridge University Press (Stirzaker)

$\triangleright$ Strang, 1991, Calculus, Wellesley-Cambridge Press (Strang)

Further reference textbooks:

$\triangleright$ Kleinberg and Tardos, 2006, Algorithm Design, Pearson/Addison-Wesley

$\triangleright$ Papadimitriou and Steiglitz, 1998, Combinatorial Optimization, Dover Books

$\triangleright$ Murphy, 2012, Machine Learning: A Probabilistic Perspective, The MIT Press

1.2. How to use these lecture notes

These notes have been designed to accompany the lectures and tutorials. The learning outcomes for each section (that is, what you should know or be able to do after reading that section thoroughly) are listed at the end of each section.

In these notes, we have highlighted specific terminology which is used in AI and ML in bold blue. These are technical terms you will need to familiarize yourself with, because they are used to shortcut discussions of technical issues, so, coursework and exam questions may be phrased in these terms. Other terms which have been borrowed from other disciplines but are not central to the topic are emphasized with italics, and other incidental that are useful or required for the discussion are highlighted in bold.

1.3. Course content

This module introduces the core concepts of artificial intelligence (AI) and machine learning (ML). The principal focus of the module will be on the underlying principles such as knowledge representation, optimization, probability, and statistical learning. On successful completion of this module, the student should be able to:

$\triangleright$ Explain and discuss a range of problems and techniques in AI and ML

$\triangleright$ Compare AI and ML techniques, describing their strengths and limitations

$\triangleright$ Apply AI and ML techniques to solve problems

1.4. Overview of AI and ML

AI and ML address challenging real-world problems which require the manipulation or processing (computation) with information (data) in some way, to come up with rational decisions given this information. In principle, all these problems could be solved manually, but humans, unless paying careful attention, easily make mistakes or get tired, so this approach is not practical for many modern, large-scale problems.

While all these problems could be solved by exhaustively generating or all possible solutions and testing them out one-by-one to select the best solution (AI), or storing all input-output patterns in memory and then just looking up the solution (ML), this is usually infeasible. A main motivation therefore of AI and ML is in coming up with "smart" solutions to solving complex problems under conditions of limited computational resources.

Examples of real-world problems which can be solved, even if approximately, using techniques from AI and ML include:

$\triangleright$ GPS navigation; finding an optimal road route, possibly including waypoints and incorporating real-time traffic updates

$\triangleright$ Scheduling limited uplink/downlink radio channels for deep space communication

$\triangleright$ Image processing; such as colorizing black and white photographs

$\triangleright$ Text-based language translation

Learning outcomes for section 1

$\triangleright$ Demonstrate understanding of the practical value of AI and ML algorithms and where they are applicable.

$\triangleright$ Be able to explain and reason broadly about the fundamental differences between symbolic AI and (statistical) ML.

Section 2 . 

Essential mathematical background

A good introduction to the necessary mathematics (ignore the examples), can be found in Gill, Section 1.2, Section 1.3, Section 1.4, Section 1.5, Section 1.6 and Section 1.7. These sections should be read now before going any further.

2.1. Essential mathematical concepts

To study the material in this course there are various background mathematical concepts and terminology that you need to know and understand. Some of these concepts are just a matter of understanding the notation, and others are derived results (theorems) which you should know. Other, more specialized mathematical concepts, notation and terminology will be introduced later in the lecture material where needed.

2.2. Mathematical notation in this module

In this course, we generally refer to natural numbers or integers using upper-case italics, i.e. $N \in \mathbb{Z}$ or $M \in \mathbb{N}$. A closed subset of the real is given by $[a, b]$, and an open set is $] a, b[$. A set of distinct values is written $\{a, b, c \ldots\}$. Names for special sets use calligraphic symbols e.g. $\mathcal{X}, \mathcal{Y}$. Real variables are written in lower-case italics, i.e. $x \in \mathbb{R}$. A sequence of $N$ variables is written $x_{1}, x_{2} \ldots, x_{N}$. Lower case letters generally refer to integer indices, as in $x_{n}$ for $n=1,2,3 \ldots, N$. A function is generally denoted $f(x)$ or $F(x)$. For differentiating once, we write $f^{\prime}(x)=\frac{d}{d x} f(x)$; the partial derivative is written $\frac{\partial}{\partial x} f(x)$. Multidimensional integration over a set $A$ is written $\int_{A} d x d y d z \ldots$ or just $\int_{A} d x$ if the context is clear. The probability of a set $A$ is denoted $\operatorname{Pr}(A)$. A probability distribution function or density function is written $P(x)$ or $p(x)$.

2.3. Plotting functions

In order to get some idea about a function, it is often very helpful to visualize it. A good plot satisfies the following criteria:

1. It should be informative and clear:

a) The scale of the axes should be adapted to the scale of the data, and/or the range of the function.

b) Axes should be labeled.

c) It should have a legend and/or a caption which explains what has been plotted and what any symbols and colours mean.

2. It should be neat and easy to read. Some tips for achieving this:

a) Use graph paper and/or a computer algebra package (such as Mathematica).

b) Draw straight lines using a ruler.

c) Distinguish different lines and points using colour and/or different line types (i.e. solid, dashed, dotted etc.)

When plotting a function by hand, you should keep the following in mind:

1. Unless otherwise restricted, a function $f(x)$ on $\mathbb{R}$ maps each value of $x$ to a unique point $y=f(x)$ on your graph, so, if there is any potential for ambiguity (for example, where a function has a jump) you should make the actual value clear using a thick point.
2. One way to plot a function qualitatively by hand, is to find a few important $x_{i}$ values (e.g. where the function crosses zero, where there are singularities or jumps etc.), then connect the points $\left(x_{i}, f\left(x_{i}\right)\right)$ as smoothly as possible. Where the points you have chosen just have convenience for your plotting, they should not be visible on the plot when it is finished.

Learning outcomes for section 2

$\triangleright$ Knowledge of the elementary mathematical concepts used in this module.

$\triangleright$ Familiarity with the mathematical notation used in this module.

$\triangleright$ How to plot mathematical functions.

Part II.

Symbolic artificial intelligence 

Section 3.

Combinatorial optimization

Relevant reference reading material for this section are MLSP, Section 1.8, Section 2.6; CLRS, Chapter 3 and $\mathbf{R} \& \mathbf{N}$, Section A.1.

3.1. Optimizing objective functions

Many problems in symbolic AI can be formulated mathematically as optimization problems. These are formulated as minimizing ${ }^{1}$ an objective function $F(X)$ with respect to a set of all possible configurations, $\mathcal{X}$ :

$$
\begin{equation*}
X^{\star}=\underset{X^{\prime} \in \mathcal{X}}{\arg \min } F\left(X^{\prime}\right) . \tag{3.1}
\end{equation*}
$$

An exact method/algorithm, is guaranteed to find (one of) the globally optimal values of (3.1). Mathematically, we say that $x$ is globally optimal if it satisfies,

$$
\begin{equation*}
F\left(X^{\prime}\right) \geq F\left(X^{\star}\right), \forall X^{\prime} \in \mathcal{X} \tag{3.2}
\end{equation*}
$$

By contrast, an approximate method/algorithm is only guaranteed to find a locally optimal value (a value which is "good enough"). Note that this locally optimal value could also be globally optimal, but the point is that we cannot be sure of this. Stated mathematically, the configuration $x$ is locally optimal if, for some subset (some restricted part) $\mathcal{N} \subset \mathcal{X}$ of the whole set of possible configurations,

$$
\begin{equation*}
F\left(X^{\prime}\right) \geq F\left(X^{\star}\right), \forall X^{\prime} \in \mathcal{N} \tag{3.3}
\end{equation*}
$$

Exact methods are typically quite slow, whereas, approximate methods might be fast but are not necessarily reliable. To appreciate the difficulty of the optimization problem, we need to understand the kind of phenomena which can be encountered in objective functions (Figure 3.1). Unless the problem is convex, that is, it has only one minima which is also the global minima, then there can be multiple local minima and/or maxima. There can also be saddle points which are locally 'flat' but which are not maxima or minima. Minima/maxima can be relatively sharp or shallow.

3.2. Exhaustive search and combinatorial explosion

Any combinatorial optimization problem (3.1) can always be solved exhaustively (sometimes called brute-force), that is, by computing $F\left(X^{\prime}\right)$ for all $X^{\prime} \in \mathcal{X}$, and then just selecting an optimal one. While this is an exact method, for most real-world problems, this method is intractable due to combinatorial explosion. As an example, consider the classic computer science problem of sorting

\footnotetext{
${ }^{1}$ We could also maximize the same objective by replacing $F(x)$ with $-F(x)$, and the problem would be unchanged.

![](https://cdn.mathpix.com/cropped/2024_07_30_8a862381e51ada05ff42g-3.jpg?height=518&width=826&top_left_y=341&top_left_x=632)

Figure 3.1.: Typical phenomena encountered in objective functions, $F(X)$ as the configuration $X$ is varied over the set $\mathcal{X}$ of possible configurations.

the set of $N=3$ numbers $\{5,18,2\}$ in ascending order; we can do this by testing every permutation (unique ordering) of the data and selecting one which is in the correct order. ${ }^{2}$ The configuration set is,

$$
\mathcal{X}=\{\{5,18,2\},\{18,5,2\},\{2,5,18\},\{5,2,18\},\{18,2,5\},\{2,18,5\}\}
$$

which we write as $|\mathcal{X}|=6$ for the problem size $N=3$, and we find that an optimal configuration is $X^{\star}=\{2,5,18\}$. It is easy to see that this is a way to guarantee finding a globally optimal solution. However, for any arbitrary problem size $N$, the number of permutations is given by the factorial, $|\mathcal{X}|=N$ ! which grows extremely rapidly as $N$ increases. For example, $10!=3628800$ and $50!\approx 3.04 \times 10^{64}$ which is an astronomically large number of possible orderings and it would take even the faster supercomputer too long to test them all on any reasonable timescale. Thus, sorting by exhaustive search is generally impractical, and we need "smarter" algorithms. This is one of the main motivations for the discipline of AI.

3.3. Complexity classes and worst case algorithm performance

Problems, such as (3.1), are often usefully classified according to their complexity class. This is a mathematical way of broadly characterizing how rapidly a function value grows with its input, using the so-called big-oh notation (asymptotic complexity), $O()$. For computational problems such as (3.1), this big-oh function tells us the worst case performance of the algorithm, either in terms of how many steps it requires - computational complexity - or in terms of how much memory it requires - space complexity. See Table 3.1. Sorting by exhaustive permutation testing has computational complexity $O(N!)$.

Complexity classes provide a mathematical expression of the behaviour of all functions within its class, as $N \rightarrow \infty$ ("asymptotically"). So, the two functions $f(N)=N^{2}+3$ and $g(N)=N^{2}$ are in the same (polynomial) class, $O\left(N^{2}\right)$, even though $g$ grows slower with $N$ than $f$.

[^0]
| Complexity class | Big-Oh notation |
| :--- | :--- |
| constant | $O(1)$ |
| logarithmic | $O(\log N)$ |
| linear | $O(N p)$ |
| log-linear | $O(N \log N)$ |
| polynomial | $O\left(N^{p}\right)$ |
| exponential | $O\left(p^{N}\right)$ |
| factorial | $O(N!)$ |

Table 3.1.: Big-oh notation for worst case complexity classes.

Generally speaking, algorithms which have polynomial computational or space complexity and better, are considered tractable (usable in practice), and problems which are exponential or worse, are considered intractable (not practically useful). The best algorithm has constant $O$ (1) complexity, which means that the number of computational steps it takes, does not in any way depend upon the size of the problem, whereas the worst algorithm has factorial $O(N!)$ complexity. ${ }^{3}$ However, this does not mean that e.g. two polynomial complexity algorithms are always as good as each other: there is a substantial practical difference between $O\left(N^{2}\right)$ and $O\left(N^{3}\right)$, for instance. The main consideration for a desirable AI algorithm, is that it has the best worst case complexity as possible.

Most AI algorithms can be organized so as to trade-off time for space (and vice-versa): the number of steps to completion can be reduced by increasing the amount of memory required, and vice-versa. For instance, if the same (or similar) problem has to be solved repeatedly, then, if we have previously found the optimal solution $x$ for some part $\mathcal{N}$ of the configuration set $\mathcal{X}$, we can store this optimal solution $x$ in memory and then next time when we are asked to find the optimal solution in $\mathcal{N}$, we can solve it in $O(1)$ steps by simply retrieving the stored solution from memory. Of course, by doing this we have to use memory which otherwise would not be required. This trick is known as memoization and is widely used in practical AI algorithms, particularly dynamic programming which is discussed later.

Learning outcomes for section 3

$\triangleright$ Express solving an AI problem through optimizing an appropriate mathematical objective with respect to a set of configurations.

$\triangleright$ Define combinatorial explosion.

$\triangleright$ Identify the complexity class of a function and the worst-case complexity class of an algorithm.

$\triangleright$ Be able to formulate and apply an exhaustive solution to a combinatorial optimization problem.

\footnotetext{
${ }^{3}$ There are even worse complexity classes than factorial, but factorial complexity is always a "hard limit" in practice.


[^0]:    ${ }^{2}$ We can express the objective function for this problem mathematically as $F(X)=0$ if $X$ is in ascending order, and $F(X)=1$ otherwise.


Section 4. 

Exact algorithms

Relevant reference reading material for this section are MLSP, Section 2.6, and CLRS, Chapter 21.

4.1. Algorithm strategies

Many algorithm strategies for constructing practical exact methods have been identified in the discipline, for instance, divide-and-conquer, branch-and-bound, and dynamic programming and variants are widely used. Every strategy involves a distinct way of organizing information and computations such as to solve the problem exactly under the best worst case time and space complexity (or at least, the best known). It is important to understand that no single strategy appears best for all problems; the usefulness and/or computational efficiency of each strategy depends heavily upon the specific structure of the problem. The choice of algorithm strategy therefore requires understanding of the specifics of the problem itself and the computational resources available to solving it. Sometimes, the availability of other information about the problem can make a big difference to the choice of strategy and resulting algorithm efficiency.

4.2. Sequential decision processes (SDPs)

In this module, we will concentrate on sequential decision processes (SDPs) as they are conceptually easy to design and describe, efficient, and furthermore, encompass many well-known AI algorithms used in practice. An SDP algorithm is a recursive process which scans $N$ input data items $x_{1}, x_{2}, \ldots, x_{N}$ in sequence, generating new configurations by extending the existing configurations using each input data item in turn, and reducing the set of candidate configurations by removing any which cannot be extended to an optimal configuration (Algorithm 4.1).

As a useful graphical aide to keep track of the calculations performed by this algorithm is a computational configuration graph. Different SDP strategies have recognizably distinct graphs. Exhaustive (brute-force) SDP algorithms have no means of applying reduction in step 3, therefore, the number of candidate configurations grows rapidly at each iteration, in a full tree structure (Figure 4.1).

At the other extreme, greedy SDP remove all but the current best candidate at each stage. As a result of this aggressive reduction, the graph is a tree with which never has more than $k$ branches, where $k$ is the worst case number of possible extensions at each stage (Figure 4.2).

Most SDPs lead to computation trees which lie somewhere in between the extremes of exhaustive and greedy, indeed. Note that, clearly, to retain algorithm exactness, the reduction step cannot remove any configuration which has the possibility of being extended to an optimal one in later iterations, so the computational efficiency of the algorithm depends heavily upon finding an effective reduction strategy.


Algorithm 4.1 Generic sequential decision process (SDP) for combinatorial optimization of AI prob-lems.
    $\triangleright$ Step 1. Initialization: Start with $n=0$, generate the root configuration(s) in the set of
    candidate configurations, $S$.

$\triangleright$ Step 2. Extension: Set $n=n+1$, and using data item $x_{n}$, extend all candidate configurations in $S$, and append these to $S$.

$\triangleright$ Step 3. Reduction: Remove any candidate configurations from $S$ which cannot be extended to an optimal configuration.

$\triangleright$ Step 4. Iteration: If $n<N$, go back to step 2 .

$\triangleright$ Step 5. Select best: Select an optimal configuration $X^{\star}$ from the remaining candidate configurations in $S$.

![](https://cdn.mathpix.com/cropped/2024_07_30_109ca619104e8f15bcc1g-2.jpg?height=478&width=1315&top_left_y=1571&top_left_x=356)

Figure 4.1.: Computational graph of a fully exhaustive sequential decision process (SDP) for solving combinatorial optimization problems in AI. Each circle is a configuration, at each stage $n$, each remaining candidate configuration at that stage is extended by the new data item $x_{n}$. After the final iteration (here, stage $n=4$ ), an optimal configuration $X^{\star}$ (orange) is selected from the remaining candidates.

![](https://cdn.mathpix.com/cropped/2024_07_30_109ca619104e8f15bcc1g-3.jpg?height=477&width=1317&top_left_y=384&top_left_x=358)

Figure 4.2.: Computational graph of a greedy sequential decision process (SDP) for solving combinatorial optimization problems in AI. Each circle is a configuration, at each stage $n$, each remaining candidate configuration at that stage is extended by the new data item $x_{n}$, and all but the next best candidate are removed from $S$. After the final iteration (here, stage $n=4$ ), an optimal configuration $X^{\star}$ (orange) is selected from the remaining candidates.

A simple example of (greedy) SDP is insertion sort (Algorithm 4.2 and Figure 4.3). The greedy reduction step is valid here because clearly, an unsorted permutation cannot be extended to a sorted permutation by any sequence of insertions, thus unsorted permutations can never lead to sorted permutations, thus the algorithm is exact. At each stage $O(N)$ new permutations are generated, thus, since there are $N$ stages, the entire algorithm has $O\left(N^{2}\right)$ time complexity. The algorithm is naive however, since there are much faster ways of constructing the permutations using a "smarter" data structure such as binary search trees which leads to a faster $O(N \log N)$ implementation (see CLRS, Section 12).

Learning outcomes for section 4

$\triangleright$ Explain and apply the sequential decision process (SDP) algorithm for exactly solving combinatorial optimization problems in AI.

$\triangleright$ Construct the computational graph for an SDP algorithm.

$\triangleright$ Explain and apply exhaustive and greedy SDP algorithms.

$\triangleright$ Determine the computational complexity of a given SDP algorithm.


Algorithm 4.2 A simple (naive) implementation of insertion sort which runs in $O\left(N^{2}\right)$ time.

$\triangleright$ Step 1. Initialization: Start with $n=0$, generate the root permutation $S=\{[]\}$.

$\triangleright$ Step 2. Extension: Set $n=n+1$, and using data item $x_{n}$, extend all candidate permutations in $S$ by inserting $x_{n}$ at every possible position in each candidate permutation, and append these new permutations to $S$, e.g. at stage $n=3,\left[x_{1}, x_{2}\right]$ would be extended to $\left[x_{3}, x_{1}, x_{2}\right],\left[x_{1}, x_{3}, x_{2}\right]$ and $\left[x_{1}, x_{2}, x_{3}\right]$.

$\triangleright$ Step 3. Reduction: Remove all candidate permutations from $S$ which are not in sorted order, if there are ties then select one arbitrarily.

$\triangleright$ Step 4. Iteration: If $n<N$, go back to step 2 .

$\triangleright$ Step 5. Select best: Select a sorted configuration $X^{\star}$ from the remaining candidate permutations in $S$.

![](https://cdn.mathpix.com/cropped/2024_07_30_109ca619104e8f15bcc1g-4.jpg?height=529&width=1315&top_left_y=1609&top_left_x=356)

Figure 4.3.: Computational graph of greedy SDP insertion sort, solving the sorting problem for the input data set $\{9,1,7,3\}$.

Section 5 . 

Dynamic programming

Relevant reference reading material for this section are CLRS, Chapter 14.

5.1. Efficient factorization and the principle of optimality

Dynamic programming (DP) is a sophisticated SDP algorithm strategy which is often able to efficiently solve difficult combinatorial AI problems in polynomial computational complexity or better. As with all SDP algorithms, the efficiency of DP depends heavily on the ability to factorize the generation of all possible configurations to reduce the number of computational steps required. Many combinatorial factorizations are known; for instance we have seen how the generation of permutations can be performed by successive insertion which means the permutation SDP takes just $O(N)$ steps. ${ }^{1}$ DP combines this efficient SDP factorization with the principle of optimality, which exploits the algebraic principle of distributivity, to avoid the need to retain only the current optimal configuration. A couple of examples, ${ }^{2}$

$$
\begin{align*}
\min (a+b, a+c) & =a+\min (b, c)  \tag{5.1}\\
\max (a \times b, a \times c) & =a \times \max (b, c) \tag{5.2}
\end{align*}
$$

For objective functions which are linear functions of the components of the configuration, we can apply the above distributivity property to derive the corresponding Bellman recursion for the DP algorithm

$$
\begin{equation*}
F\left(X_{n}^{\star}\right)=\min _{X^{\prime} \in S_{n}} F\left(X^{\prime}\right) \tag{5.3}
\end{equation*}
$$

where $S_{n}$ contains extensions of the optimal configuration $X_{n-1}^{\star}$, accompanied by the recursion for the optimal configuration itself,

$$
\begin{equation*}
X_{n}^{\star}=\underset{X^{\prime} \in S_{n}}{\arg \min } F\left(X^{\prime}\right), \tag{5.4}
\end{equation*}
$$

for all $n=1,2, \ldots, N$. To get the recursion started at $n=0$, we need the root configuration $X_{0}^{\star}$ and $F\left(X_{0}^{\star}\right)$; their values depend upon the specific DP algorithm.

It helps to explain (5.3) in a bit more detail. At each stage $n$, the candidate configurations are obtained by extending the optimal configuration $X_{n-1}^{\star}$ in the previous stage, so the optimal value of the objective function $F\left(X_{n}^{\star}\right)$ and corresponding optimal configuration $X_{n}^{\star}$, can be obtained using only the optimal configuration from the previous stage. ${ }^{3}$ If the factorized SDP takes $O(N)$ steps and if there are at most $k$ ways to extend each configuration, the complete DP algorithm has time complexity

[^0]$O(N k)$ i.e. it is linear in the size of the input. This is usually remarkably efficient because $k$ is often small, and DP is therefore used widely in AI and ML.

Of course, whether it is possible to find a factorization with small $k$ and thus a fast Bellman recursion for a particular AI optimization problem is not an easy problem in itself to solve, and requires some ingenuity to devise.

5.2. Example: maximum sum tail subsequences

As an example of using DP in practice, let us look at an example of the maximum sum tail subsequence problem. The tail subsequences of a list with three items $\left[x_{1}, x_{2}, x_{3}\right]$ are $\left[x_{3}\right],\left[x_{2}, x_{3}\right]$ and $\left[x_{1}, x_{2}, x_{3}\right]$. The problem is to (efficiently) find the tail subsequence with maximum sum, i.e. for three items, select between $x_{3}, x_{2}+x_{3}$ and $x_{1}+x_{2}+x_{3}$. This problem arises in logistics: if we have a queue of containers at a dock to load onto a cargo ship which must be loaded in order, and shipping each container leads to either a profit for the the shipping company (positive value data item) or loss for the company (negative value item), then the shipping company will want to maximize the profit by emptying the optimal amount of the queues in the container, onto the ship.

To express the optimization problem mathematically, we need a representation of the load (a configuration), $X$. Here we use lists, so $X=[4,5,-3]$ is a load selecting one of the tail subsequences of the input list $[2,7,4,5,-3]$. Using this representation, the optimization problem is,

$$
\begin{equation*}
X^{\star}=\underset{X^{\prime} \in \mathcal{X}}{\arg \max }\left(\sum_{x^{\prime} \in X^{\prime}} x^{\prime}\right) \tag{5.5}
\end{equation*}
$$

where here $\mathcal{X}$ represents the set of all possible tail subsequences of the list $\left[x_{1}, \ldots, x_{N}\right]$. So, the problem is expressed as computing the sum of all possible subsequence loads and selecting the one with the largest value. ${ }^{4}$

To solve this using DP, we need an efficient factorized SDP for generating items in the set $\mathcal{X}$. This can be done using the following simple procedure: starting with the empty list [], at each subsequent stage, either add an empty list [] to the set of candidate configurations, or append the next item in the input $x_{n}$ onto the end of all current candidate lists (Figure 5.1). For DP to be applicable, we also require the objective function to be linear in the configuration update. In this case, appending an item to a configuration corresponds to adding the value of this item to the total objective function value for that configuration, which is a linear operation as required.

Finally, since an empty load will have zero profit/loss, this provides an enough information to write down the Bellman recursion for the optimal profit of the shipping company,

$$
\begin{equation*}
P_{n}^{\star}=\max \left(0, P_{n-1}^{\star}+x_{n}\right), \tag{5.6}
\end{equation*}
$$

for $n=1,2, \ldots, N$ with $P_{0}^{\star}=0$ to start the recursion at $n=0$. The corresponding optimal choice of list of containers to load is given by,

$$
X_{n}^{\star}= \begin{cases}{[]} & P_{n-1}^{\star}+x_{n} \leq 0  \tag{5.7}\\ \text { append } x_{n} \text { to the end of } X_{n-1}^{\star} & \text { otherwise }\end{cases}
$$

See Figure 5.2 for a toy example illustrating the sequence of steps in the above Bellman recursion. In terms of computational complexity, this algorithm runs in $O(N)$ time, whereas, to solve the problem

[^1]![](https://cdn.mathpix.com/cropped/2024_07_30_8b6c3679603def0a6043g-3.jpg?height=578&width=1189&top_left_y=365&top_left_x=431)

Figure 5.1.: Computational graph of a fully exhaustive sequential decision process (SDP) for generating all tail subsequences of a length four input sequence $\left[x_{1}, \ldots, x_{4}\right]$.

exhaustively would require $O\left(N^{2}\right)$ complexity. This shows how DP can lead to very efficient solutions to discrete optimization problems in AI.

Learning outcomes for section 5

$\triangleright$ Explain the principles of dynamic programming (DP) for exactly solving combinatorial optimization problems in AI.

$\triangleright$ Recognize, design and apply an appropriate Bellman recursion for specific DP algorithms.

$\triangleright$ Determine the computational complexity of a DP algorithm.

![](https://cdn.mathpix.com/cropped/2024_07_30_8b6c3679603def0a6043g-4.jpg?height=573&width=1196&top_left_y=1007&top_left_x=427)

Figure 5.2.: Computational graph of DP for solving the maximum sum tail subsequence, for the input data list $[2,-3,-4,6]$. The numbers after the subsequences refer to the sum of that subsequence, e.g. $[2,-3]:-1$ means that the subsequence $[2,-3]$ has total sum -1 . The optimal solution is found at the final stage, $X^{\star}=[6]$ with $F\left(X_{4}^{\star}\right)=6$.


[^0]:    ${ }^{1}$ Note that this does not mean that the corresponding space complexity of this factorization is linear in $N$. Indeed, the for permutations the space complexity grows with $N!$.

    ${ }^{2}$ This is exactly the same algebraic principle as $a b+a c=a \times(b+c)$ but in a different algebra!

    ${ }^{3}$ This may help to explain why (5.3) is a recursive equation.

[^1]:    ${ }^{4}$ For consistency here we could also minimize the negative sum of values, but this is entirely equivalent to maximizing the positive sum.


Section 6. 

Approximate combinatorial algorithms

Relevant reference reading material for this section: R\&N, Section 4.1 and MLSP, Section 2.6.

6.1. Global and local optima

Exact optimization methods are completely reliable, but typically quite slow, whereas approximate methods can be fast but not necessarily reliable. Approximate methods cannot guarantee finding an optimal solution to (3.1), only a solution which is 'good enough' for a specific purpose. Various algorithm strategies for approximate optimization exist: stochastic (randomized), greedy ${ }^{1}$, tabu search, genetic optimization, particle swarm optimization (PSO), simulated annealing, local beam search and many others. Approximate methods usually start with some guess configuration and aim to find a configuration which has an objective function value which is at least better than the starting guess.

Without additional information about a specific problem, a few things can be said in general about approximate optimization methods (see Figure 3.1):

1. In the search for an improvement over the starting configuration, they can become "trapped" in local optima which are indistinguishable from global optima,
2. The success of any approximate algorithm can be heavily dependent upon the starting guess: a good guess near to the global optima will be better than a bad guess close to a local optima and/or far from the global one, however, since there is no way in general of discriminating global from local optima we cannot know whether the starting guess is good or bad (without comparing different starting guesses),
3. It is usually not possible to know how good a local optima is, by comparison to the (unknown) global optima.
4. If the algorithm stops at an optima, this may actually be the global optima, but there is no way to know this in general.

Some sophisticated stochastic algorithms have convergence guarantees, for example, Markov chain Monte Carlo (MCMC) methods such as simulated annealing (which we will investigate below) but only in the limit of an infinite number of iterations. Even in this situation, the number of iterations required to find a configuration whose objective is within some required tolerance of the optimal solution, is typically unknown.

\footnotetext{
${ }^{1}$ Here greedy refers to approximate greedy, not exact greedy.


Algorithm 6.1 Approximate greedy search for combinatorial optimization of AI problems.
  
 $\triangleright$ Step 1. Initialization: Start with iteration $n=0$, select an initial candidate configuration $X_{0}$, choose a maximum number of iterations $R$.

$\triangleright$ Step 2. Neighbourhood search: Find the configuration $X^{\prime}$ within the configuration neighbourhood $\mathcal{N}\left(X_{n}\right)$ with smallest objective function value $F\left(X^{\prime}\right)$, and set $X_{n+1}=X^{\prime}$. If $F\left(X^{\prime}\right) \geq F\left(X_{n}\right)$, then terminate with $X^{\star}=X_{n}$.

$\triangleright$ Step 3. Iteration: Set $n=n+1$, and while $n \leq R$, go back to step 2 , otherwise exit with solution $X^{\star}=X_{n}$.

6.2. Approximate greedy search

In this section we will explore a simple but often surprisingly effective, approximate method known as greedy approximate search, hill-climbing or iterative improvement (Algorithm 6.1). It is based on the idea of performing many searches which are optimal within a certain neighbourhood of the current configuration, and using the best known local configuration to centre the next local search. When this algorithm becomes trapped at some optima in the objective function (as it will eventually), it terminates with the last best known configuration.

To use the algorithm we need a definition of configuration neighbourhood, $\mathcal{N}(X)$ of some configuration $X$. There are many ways to measure distance (the mathematical name for which is metric) between configurations, one widely used metric is the Hamming distance $d\left(X, X^{\prime}\right)$ which counts the number of elements of the two configurations $X$ and $X^{\prime}$ which differ. For instance, if the configurations are permutations, then $d\left(X, X^{\prime}\right)$ would measure the number of places in which the permutations disagree; when they are the same permutation then $d(X, X)=0$. Thus a combinatorial neighbourhood based on this metric ${ }^{2}$ would be defined mathematically as

$$
\begin{equation*}
\mathcal{N}(X)=\left\{X^{\prime} \in \mathcal{X}: d\left(X, X^{\prime}\right) \leq r\right\} \tag{6.1}
\end{equation*}
$$

for some given maximum Hamming distance $r>0$.

Some facts about the behaviour of this algorithm can be inferred directly. Firstly, by design, the sequence of candidate configurations at each iteration $X_{0}, X_{1}, X_{2}, \ldots$ will have decreasing objective function value, i.e. $F\left(X_{n+1}\right)<F\left(X_{n}\right)$ for all $n=0,1,2, \ldots$. Because the algorithm is approximate, there is no guarantee that the final value $X^{\star}$ coincides with the global minima. The algorithm could finish before $n=R$, but we cannot predict in advance when it will terminate. There is trade off between the neighbourhood size $r$, and the efficiency of the algorithm. If $r$ is large, the search will be slow, but since each iteration covers more of the space $\mathcal{X}$, it is more likely to find the global optima. If $r$ is small, each iteration will be fast, but the more likely the algorithm will be to get trapped in local optima and never find the global optima.

6.3. Example: maximum sum combinations

To illustrate greedy search, we will investigate the maximum sum combination problem, that is, finding a size- $M$ combination of the $N$ input data items, such that the sum of this combination is maximized. For instance, with input data $x=[0.5,-0.1,0.3,-0.2]$, the optimal size 2 combination is

\footnotetext{
${ }^{2} \mathrm{Or}$ any other combinatorial metric, in fact.

| Iteration | Configuration $X_{n}$ | Objective value $F\left(X_{n}\right)$ |
| :--- | :--- | ---: |
| $n=0$ (initialize) | $\{0.33,-0.19,-0.59,-0.14\}$ | -0.59 |
| $n=1$ | $\{0.33,-0.19, \mathbf{2 . 1 8},-0.14\}$ | 2.18 |
| $n=2$ | $\{0.33, \mathbf{1 . 1 9}, 2.18,-0.14\}$ | 3.56 |
| $n=3$ | $\{\mathbf{1 . 1 9}, 1.19,0.33,2.18\}$ | 4.89 |
| $n=4$ (terminate) | $\{1.19,1.19, \mathbf{1 . 0 7}, 2.18\}$ | 5.63 |

Table 6.1.: Example output from approximate greedy search, Algorithm 6.1, applied to the maximum sum combination problem, for input data size $N=40$ and combination size $M=4$. The neighbourhood search uses Hamming distance with radius $r=1$. In this run, the globally optimal solution is found by the algorithm after 4 iterations. The bold numbers at each stage highlight values from the input which are replaced during each stage. Input data values are $x=[-0.43,-1.67,0.13,0.29,-1.15$, $1.19,1.19,-0.04,0.33,0.17,-0.19,0.73,-0.59,2.18,-0.14,0.11,1.07,0.06,-0.10,-0.83]$.

$\{0.5,0.3\}$ with sum $0.8{ }^{3}$ This problem comes up in many real-world situations. As an example, in graphic design, you have to fill all of the advertising slots of a magazine with advertisements. There are $N$ advertisers and the profit the publisher makes from the $n$-th sponsor is $x_{n}$ (some advertisements are loss leaders - a small loss is made on these - but the publisher is forced to fill out every advertising page in the magazine). Maximizing profit from the magazine is a matter of solving an instance of the maximum sum combination problem. The problem is specified mathematically as,

$$
\begin{equation*}
X^{\star}=\underset{X^{\prime} \in \mathcal{X}}{\arg \max } \sum_{x^{\prime} \in X^{\prime}} x^{\prime} \tag{6.2}
\end{equation*}
$$

where $\mathcal{X}$ is the set of all size $M$ combinations of the data items $x_{1}, \ldots, x_{N}$.

For the search neighbourhood, we will use the Hamming distance which in this case is the number of items in two combinations $X$ and $X^{\prime}$ which differ. To see how good the result of this approximate search is, for small $N$ and $M$ at least, we can find the global optima using exhaustive search. Computational experiments confirm that for small $N$ and $M$, for Hamming distance $r=1$, approximate greedy search strategy apparently almost always finds the global maxima in a handful of iterations (see Table 6.1).

From these experiments it seems that hill-climbing is a good solution to the problem. However, in fact this problem can be solved exactly by selecting the top $M$ items in the input data, which requires sorting the data with worst case computational complexity of $O(N \log N)$. So, the computational efficiency of Hamming-distance based greedy approximation for this particular problem is doubtful: its success comes at the expense of very slow neighbourhood search. We can speed up the neighbourhood search by selecting a subset of the neighbourhood $\mathcal{N}\left(X_{n}\right)$ at each iteration; when we do this with a random selection of $k$ candidate configurations for instance, the algorithm almost always fails to find the globally optimal solution, although the solution is often quite close to optimal.

\footnotetext{
${ }^{3}$ This is a simplified instance of the more complex knapsack or bin-packing problem.

Learning outcomes for section 6

$\triangleright$ Identify and explain the implications of local and global optima in combinatorial optimization for problems in AI.

$\triangleright$ Provide the steps in the approximate greedy search algorithm and apply the algorithm to combinatorial optimization problems.

$\triangleright$ Determine the computational complexity of a specific application of approximate greedy search.

Section 7. 

Simulated annealing

The recommended reference reading material for this section is $\mathbf{R} \& \mathbf{N}$, Section 4.1 and MLSP, Section 2.6 .

7.1. Escaping local optima

Approximate greedy search, Algorithm 6.1, is not guaranteed to find the global optima, because it can become trapped in local optima. This happens because it only improves the value of the objective at each iteration. If we could escape these local optima, the algorithm could (in principle) find the global optima, at least eventually. Simulated annealing (Algorithm 7.1) is an approximate search method which can accept an updated configuration with worse objective function value. ${ }^{1}$

The probability of accepting a 'bad' step is inspired by the Boltzmann distribution from the discipline of thermodynamics:

$$
\begin{equation*}
\operatorname{Pr}(\text { accept })=\exp \left(-\left[F\left(X^{\prime}\right)-F\left(X_{n}\right)\right] \frac{n}{k R}\right) \tag{7.1}
\end{equation*}
$$

The worse $F\left(X^{\prime}\right)$ is in the search neighbourhood $\mathcal{N}\left(X_{n}\right)$, the smaller the probability of accepting this bad step (the algorithm tries to take the least worst bad steps). Furthermore, as $n$ gets closer to the maximum number of iterations, $R$, the probability of accepting a bad step decreases. This means that simulated annealing eventually behaves like simple approximate greedy search, only improving the objective, and finally converging on a value $X^{\star}$ (which is not guaranteed to be optimal). The

[^0]
Algorithm 7.1 Simulated annealing for combinatorial optimization of AI problems (here stated for
the case of objective function minimization rather than maximization).

$\triangleright$ Step 1. Initialization: Select a starting candidate configuration $X_{0}$ and set $X^{\star}=X_{0}$, set iteration number $n=0$, set $F^{\star}=\infty$, choose annealing schedule $k>0$, and maximum number of iterations $R$

$\triangleright$ Step 2. Neighbourhood search: Choose any configuration $X^{\prime}$ from $\mathcal{N}\left(X_{n}\right)$. If $F\left(X^{\prime}\right)<$ $F\left(X_{n}\right)$, set $X_{n+1}=X^{\prime}$, otherwise choose a random number $q$ from $[0,1]$, and if $q>$ $\exp \left(-\left[F\left(X^{\prime}\right)-F\left(X_{n}\right)\right] \frac{n}{k R}\right)$ then set $X_{n+1}=X^{\prime}$

$\triangleright$ Step 3. Maintain best configuration: If $F\left(X_{n+1}\right)<F^{\star}$, set $F^{\star}=F\left(X_{n+1}\right)$ and $X^{\star}=X_{n+1}$

$\triangleright$ Step 4. Iteration: Set $n=n+1$, and while $n<R$, go back to step 2 , otherwise exit with solution $X^{\star}$.

![](https://cdn.mathpix.com/cropped/2024_07_30_0b467cdabd5d514b200dg-2.jpg?height=553&width=830&top_left_y=306&top_left_x=593)

Figure 7.1.: Simulated annealing applied to the maximum sum combination problem. Here, the data has $N=20$ items and the configurations are combinations of size $M=4$. The annealing schedule was $k=0.1$ and the maximum number of iterations $R=100$. On this run, the iteration converges on the exact global configuration.

parameter $k>0$ is known as the annealing schedule. If $k$ is large, the probability of taking a bad step decreases slowly and convergence will take a large number of iterations. Conversely, if $k$ is small, convergence is rapid but the algorithm is more likely to get trapped in a local optima, behaving essentially as approximate greedy search.

7.2. Example: annealing better maximum sum combinations

In the previous section, we observed that the Hamming distance neighbourhood search for the approximate greedy algorithm is accurate (i.e. mostly able to find the global optima) but intractable, but that when the neighbourhood size was reduced to make the algorithm more tractable, the algorithm was no longer accurate. The reason for this loss of accuracy is that the reduced neighbourhood search causes the greedy algorithm to get stuck. Simulated annealing has much simpler neighbourhood search which only requires selecting one configuration, and can avoid getting stuck through backwards steps. Although taking very many more iterations than approximate greedy search, computational experiments on the same problem and same data as approximate greedy search, is quite effective and often converges on the exact global solution (Figure 7.1).

We can see from the run shown in Figure 7.1 that although simulated annealing tends to get trapped in local maxima however, it eventually escapes these local maxima to quickly find a new, better configuration, eventually reaching the global optima.

Learning outcomes for section 7

$\triangleright$ Explain the steps in the simulated annealing algorithm.

$\Delta$ Compare the advantages and disadvantages of simulated annealing algorithm for approximate global optimization of AI problems.


[^0]:    ${ }^{1}$ The name of this method comes from the physical manufacturing process of slowly cooling an amorphous crystalline solid (such as glass), to strengthen it by removing structural defects.

Section 8. 

Logic

Relevant reference reading material for this section is $\mathbf{R} \& \mathbf{N}$, Section 7.3 and Section 7.4.

8.1. Propositional calculus

Any computational system is, at root, an application of logic to data about problems in the real world. This application of logic is essential to any form of rational decision-making and reasoning, and AI systems must be rational if they are to be reliable. Logical inference involves combining facts about the world using the rules of logic, to draw correct conclusions on other facts about the world. For instance, the chain of common-sense reasoning which allows us to combine the rule 'if it is sunny then it is hot' with the fact that today is sunny, to conclude that today is hot, is an example of rational logical inference.

In order to automate logical reasoning like this computationally, we use propositional calculus to formalize the process mathematically. This is a type of algebra with extremely similar algebraic rules to the rules of algebra in ordinary arithmetic computations. The syntax of the algebra, that is, the rules by which the symbols in the algebra are combined, is identical to that in ordinary algebra, but instead of using plus, times and ordinary numbers, makes use of special symbols for logical operations or logical connectives, see Table 8.1. For example, the logical expression (sentence) ' $\operatorname{Tr}$ re $\vee(x \wedge y)$ ' is syntactically correct (well-formed), but ' $\vee \wedge T r u e)(x y$ ' is not, in exactly the same way that ' $1 \times(x+y)$ ' is a valid algebraic expression 'but $+\times 1$ ) ( $x y$ ' is not.

Unlike arithmetic which uses ordinary numbers, the semantics of variables and their values in propositional calculus are Boolean, that is, they can only take on one of the truth values True or False. This means that all the functions which operate on those variables are Boolean functions, which can only take and output Boolean values. The semantics of Boolean functions can therefore be explicitly listed using truth tables, see Table 8.1 for the truth tables of the logical connectives. The possible truth values of an expression in propositional calculus can thus always be given using truth tables.

Propositional variables, which represent some state of the world, are called atomic (logical) statements or simple and are denoted in expressions by the upper case italic variable names $P, Q, R$ etc. Examples: $R=$ 'raining today' or $D=$ 'the car is in the driveway'. The Boolean (logical) constants True and False are also atomic statements. Expressions which are made by combining two or more atomic expressions using connectives are called complex statements or compound statements. Examples of compounds statements are $R \wedge \neg D$ which is interpreted as 'it is raining today and the car is not in the driveway'.

If we have the truth values of all the variables in a propositional expression, then we can evaluate the expression to find its truth value. We do this by replacing the syntactic elements by their semantic counterparts, then using truth tables recursively until we have a single truth value. This is exactly the same process as performing arithmetic in order to evaluate an algebraic expression, but using logical

| Symbol | Meaning | Truth table |  |  |
| :---: | :---: | :---: | :---: | :---: |
| $\neg$ | Not (negation) | $x$ | $\neg x$ |  |
|  |  | False | True |  |
|  |  | True | False |  |
| $\wedge$ | And (conjunction) | $x$ | $y$ | $x \wedge y$ |
|  |  | False | False | False |
|  |  | False | True | False |
|  |  | True | False | False |
|  |  | True | True | True |
| V | Or (disjunction) | $x$ | $y$ | $x \vee y$ |
|  |  | False | False | False |
|  |  | False | True | True |
|  |  | True | False | True |
|  |  | True | True | True |
| $\Longrightarrow$ | If ... then (implication) | $x$ | $y$ | $x \Longrightarrow y$ |
|  |  | False | False | True |
|  |  | False | True | True |
|  |  | True | False | False |
|  |  | True | True | True |
| $\Longleftrightarrow$ | If and only if (equality) | $x$ | $y$ | $x \Longleftrightarrow y$ |
|  |  | False | False | True |
|  |  | False | True | False |
|  |  | True | False | False |
|  |  | True | True | True |

Table 8.1.: The logical connectives of propositional calculus.
connectives rather than the usual arithmetic operations. For example, if $H=$ 'the house is occupied' and we have the proposition $\neg R \wedge \neg D \Longrightarrow \neg H$, and if in the real world, $R=$ True, $D=$ True and $H=$ True, we can calculate, ${ }^{1}$

$$
\begin{align*}
\neg R \wedge \neg D \Longrightarrow \neg H & =\neg \text { True } \wedge \neg \text { True } \Longrightarrow \neg \text { True } \\
& =\text { False } \wedge \text { False } \Longrightarrow \text { False } \\
& =\text { False } \Longrightarrow \text { False }  \tag{8.1}\\
& =\text { True }
\end{align*}
$$

so we can conclude that under these specific conditions, the proposition holds (i.e. it is true). We can interpret this as the claim that 'if it is not raining today and the car is not in the driveway then the house is empty', is true in this case.

A set of conditions like this in the real world which give assignments of actual truth values to variables, is called a (logical) model. If some proposition $A$ is true when the variables are set to model $M$, then we say that $M$ satisfies $A$. The set of all models under which $A$ is true is written $M(A)$. A very simple example: for the proposition $P=A \vee$ True, the set of models $M(P)=$ $\{A=$ False,$A=$ True $\}$.

8.2. Logical inference

The main purpose of propositional calculus is to use it to automate the process of logical reasoning, i.e. making correct logical inferences. We want to show that a proposition $Q$ is a necessary consequence of $P$, or equivalently $P$ entails $Q$,

$$
\begin{equation*}
P \vDash Q \text { if and only if } M(P) \subseteq M(Q) \text {. } \tag{8.2}
\end{equation*}
$$

In words (8.2) means that every model in which $P$ is true, is also true for $Q$.

Assume we have some set of facts and logical rules, which we will represent as propositions, which we will call a knowledge base, which we will give the label $K B$, the truth value of which is just the conjunction of all the propositions it contains. We want to be able to automate the process of deciding whether a proposition $Q$ is a necessary consequence of $K B$, i.e. to test whether $K B \vDash Q$. The simplest way to do this is just to calculate (8.2) directly: compute $M(K B)$ and $K(Q)$ and if $M(K B) \subseteq$ $M(Q)$ then our proposition is a necessary consequence of the knowledge base. This procedure, called model checking (Algorithm 8.1), is obviously perfectly reliable (we say the procedure is sound and complete), because it cannot fail to correctly identify entailment where it holds. We can see this as being a case of exhaustive search because it solves the problem by testing all possible configurations of the variables. Because of this, the complexity of model checking is exponential in $N$, the number of variables, in fact one has to check exactly $2^{N}$ possible combinations.

8.3. Example: automated medical decision-making

As an example of automated propositional reasoning, consider the (realistic but heavily simplified) Parkinson's medical knowledge base. It contains the following facts, $C=$ 'Satisfying diagnostic criteria', $O=$ 'Over age 60' and $P=$ 'Suspect Parkinson's disease', along with the differential diagnostic rule that satisfying the diagnostic criteria and being over 60 , implies a suspicion of Parkinson's, $D=$

[^0]
Abstract

Algorithm 8.1 Logical model checking for testing entailment $P \vDash Q$. $>$ Step 1. Exhaustive search: For all propositional variables $A, B, C$ etc. upon which propositions $P$ and $Q$ depend, evaluate $P$ and $Q$ under all possible combinations of truth values of those variables.


$\triangleright$ Step 2. Compute models. Using this find the set of models $M(P)$ where $P$ holds and $M(Q)$ where $Q$ holds.

$\triangleright$ Step 3. Test subset: If $M(P) \subseteq M(Q)$ then return $P \vDash Q$ is true, false otherwise.

| $C$ | $O$ | $P$ | $(C \wedge O) \Longrightarrow P$ | $\neg O \Longrightarrow \neg P$ |
| :--- | :--- | :--- | :--- | :--- |
| False | False | False | True | True |
| False | False | True | True | False |
| False | True | False | True | True |
| False | True | True | True | True |
| True | False | False | True | True |
| True | False | True | True | False |
| True | True | False | False | True |
| True | True | True | True | True |

Table 8.2.: Model checking applied to a simplified Parkinson's disease diagnostic knowledge base $K B$ with the facts $C=$ 'Satisfying diagnostic criteria', $O=$ 'Over age 60 ' and $P=$ 'Suspect Parkinson's disease', along with the differential diagnostic rule that satisfying the diagnostic criteria and being over 60 , implies a suspicion of Parkinson's, $K B=D=(C \wedge O) \Longrightarrow P$. The query $Y=\neg O \Longrightarrow \neg P$ asks whether being under 60 implies no suspicion of Parkinson's. The knowledge base cannot tell us this because $K B \vDash Y$ does not hold.

$(C \wedge O) \Longrightarrow P$. There are $2^{3}=8$ possible models, so it is possible to apply Algorithm 8.1 by hand. We would like to know if the knowledge base $K B=D$ entails whether being under age 60 removes the suspicion of Parkinson's, which expressed in terms of the knowledge base is the proposition $Y=\neg O \Longrightarrow \neg P$. Table 8.2 checks $K B \vDash Y$, and we can see from this that $M(K B)$ is true in cases where $M(Y)$ is not, thus it is not true that $M(K B)$ is contained in $M(Y)$ so the inference that being under 60 implies no suspicion of Parkinson's, does not follow from the knowledge base. Indeed, this mirrors what is known in the real world; although rare, there exist so-called young onset Parkinson's cases, a famous example being the actor Michael J. Fox.

Learning outcomes for section 8

$\triangleright$ Explain the syntax and semantics of propositional calculus.

$\triangleright$ Use the rules of propositional calculus to calculate the truth value of simple propositional expressions.

$\triangleright$ Explain the model checking algorithm, and apply it to test entailment in simple logical knowledge bases.


[^0]:    ${ }^{1}$ Like algebra, there are precedence rules to remove ambiguity: first evaluate not, then and, then or, then implication, then equality. Brackets override the ordering.


Part III.

Machine learning 

Section 9 .

Machine learning and gradient descent

Recommended reference reading material for this section is MLSP, Section 2.3, Section 6.4, R\&N, Section 18.6, PRML, Section 3.1 and H\&T, Section 3.2. You should also read Gill, Sections 5.1-5.4 for the essential mathematics of the derivative and gradient of a function.

9.1. Overview of machine learning

Whereas symbolic AI deals with certainties and exact computational rules, machine learning (ML) extracts "learns") ${ }^{1}$ patterns from data which may be uncertain and uses these to make predictions. It is therefore a form of inductive reasoning. Given some training data, ML algorithms are trained by minimizing an error (objective, loss) function $F(w)$ which depends upon the continuous $w$ parameters of the ML model (along with some possibly discrete-valued parameters); the smaller the error function on the training data, the better. It then uses the trained parameters $w^{\star}$ in the model, to make predictions about new unseen data, the test data. ML algorithms are usually categorized according to the availability of labeled data: supervised, unsupervised, self-supervised, transfer learning and many others.

9.2. Training ML algorithms using sequential gradient descent (SGD)

Estimating the best model parameters in a ML problem, involves optimizing the error function $F(w)$ with respect to the continuous parameters $w$ of the algorithm over the space of all possible algorithm values, $\mathcal{W}$ :

$$
\begin{equation*}
w^{\star}=\underset{w^{\prime} \in \mathcal{W}}{\arg \min } F\left(w^{\prime}\right) \tag{9.1}
\end{equation*}
$$

Typically, the space $\mathcal{W}$ is just the Euclidean space $\mathbb{R}^{D}$, where $D$ is the number of parameter dimensions. Typically, the objective function (9.1) is not convex and has a great many local optima, so the training method does not necessarily seek to find the global optima for $(9.1) .^{2}$

One of the most widely used methods for attempting to solve (9.1) is sequential gradient descent (SGD) (Algorithm 9.1). This is a general algorithm for finding a value of the ML model parameters $w$ such that error function $F(w)$ is as small as possible, expressed as a condition on the multivariable gradient, $F_{w}(w)=0^{3}$, where $F_{w}$ is the partial derivative of $F$ with respect to the parameter vector $w$,

[^0]
Algorithm 9.1 Sequential gradient descent (SGD) for optimization of model parameters in machine learning.

$\triangleright$ Step 1. Initialization: Select a starting candidate $w_{0}$, set iteration number $n=0$, choose convergence tolerance $\epsilon>0$ and learning rate $\alpha>0$

$\triangleright$ Step 2. Gradient descent step: Compute new model parameters, $w_{n+1}=w_{n}-\alpha F_{w}\left(w_{n}\right)$

$\triangleright$ Step 3. Convergence test: Compute new error function $F\left(w_{n+1}\right)$ and loss function improvement $\Delta F=\left|F\left(w_{n+1}\right)-F\left(w_{n}\right)\right|$, and if $\Delta F<\epsilon$, exit with optimal solution $w^{\star}=w_{n+1}$

$\triangleright$ Step 4. Iteration: Set $n=n+1$, go back to step 2 .

$$
F_{w}(w)=\left[\begin{array}{c}
\frac{\partial F}{\partial w_{1}}(w)  \tag{9.2}\\
\frac{\partial F}{\partial w_{2}}(w) \\
\vdots \\
\frac{\partial F}{\partial w_{D}}(w)
\end{array}\right]
$$

which is just the vector of gradients in each dimension separately.

The basic idea is this: starting with a guess for $w_{n}$, take a "step" in the direction of steepest descent of the loss function, $-F_{w}(w)$, and take this as a better guess $w_{n+1}$. The size of the step, $\alpha$ $>0$, is called the learning rate which determines how quickly the minimum is reached. This simple procedure is, of course, not guaranteed to find the global minimum, unless the error function is convex with respect to $w$ which with most ML algorithms is generally not the case. More advanced versions of this algorithm are very widely used in modern ML.

One of the difficulties with SGD is that it is possible to overshoot a minimum if the learning rate is too large. As a result the error function on the next iteration can be larger than on the previous one. The next step occurs may be in nearly the opposite direction, potentially overshooting again, and so on. This is the reason for computing the absolute value $\|$ of the improvement, $\Delta F$ : if we only computed the difference in loss function values at each step, we could end up with negative improvement and never terminate. Furthermore if $\alpha$ is above a critical threshold the algorithm might diverge, that is the objective function increases without bound. Choosing the learning rate parameter is, therefore critical and often more of an art than a science with complex modern ML algorithms.

9.3. Example: linear regression

A regression model learns to fit a curve $f(x, w)=y$ through pairs of training data $\left(x_{n}, y_{n}\right)$ for $n=1,2, \ldots, N$ where $w$ are the model parameters. Once fitted to the training data, we can use the best parameters $w^{\star}$ and some input data $x$, to make predictions $y$. The tacit hypothesis is that there is some (continuous) relationship between input and output variables $x$ and $y$. This is a supervised technique: given input data and corresponding labeled output data, regression attempts to find a curve which fits the training data best.

The specific hypothesis depends upon the chosen form of regression function. For instance, a linear model for $D=1$ is a straight line, and for $D>1$ a hyperplane. This is called a linear regression model,

![](https://cdn.mathpix.com/cropped/2024_07_30_e5725121dc82b5a4e3a7g-4.jpg?height=735&width=935&top_left_y=318&top_left_x=549)

Figure 9.1.: Linear regression in $D=2$ (with first coordinate values set to 1 ) as single dimension curve fitting, showing the interpretation of the sum-of-squares error $F(w)=\sum_{i=1}^{N}\left(w^{T} x_{i}-y_{i}\right)^{2}$, where the terms $w^{T} x_{i}-y_{i}$ are the perpendicular distances between the best fit line (orange) and the coordinates of the data points (black dots).

$$
\begin{align*}
f(x, w) & =w_{1} x^{1}+w_{2} x^{2}+\cdots+w_{D} x^{D} \\
& =w^{T} x, \tag{9.3}
\end{align*}
$$

where we use superscript to index each dimension (row) of the vector $x .{ }^{4}$ Using this, we can write down the square loss function, which is the square of the difference between the model prediction $w^{T} x_{n}$ on each data item $x_{n}$ and the corresponding labeled item in the data, $y_{n}$,

$$
\begin{equation*}
F(w)=\sum_{i=1}^{N}\left(w^{T} x_{i}-y_{i}\right)^{2} \tag{9.4}
\end{equation*}
$$

which quantifies the quality of the fit of the linear regression model to the data. In $D=2$ dimensions, if we fix the first data coordinate to 1 (which becomes the intercept of the line on the plot), we can visualize the model fit on an $x-y$ plot which shows that the error is the sum of the squares of the perpendicular distances to the best fit line (Figure 9.1).

As an example, let us apply SGD to this loss function. To do this, we need the gradient of (9.4), which is, ${ }^{5}$

$$
\begin{equation*}
F_{w}(w)=2 \sum_{i=1}^{N}\left(w^{T} x_{i}-y_{i}\right) x_{i} \tag{9.5}
\end{equation*}
$$

[^1]
| Iteration | Parameters $w_{n}$ | Error function value $F\left(w_{n}\right)$ | Improvement $\Delta F$ |
| :--- | :--- | ---: | ---: |
| $n=0$ (initialize) | $[-0.09,-1.60]$ | 69.24 | $\mathrm{~N} / \mathrm{A}$ |
| $n=1$ | $[0.10,-0.07]$ | 28.20 | 41.04 |
| $n=2$ | $[-0.41,0.60]$ | 16.46 | 11.73 |
| $n=3$ | $[-0.28,1.04]$ | 13.08 | 3.38 |
| $n=4$ | $[-0.47,1.22]$ | 12.10 | 0.99 |
| $n=5$ | $[-0.40,1.35]$ | 11.80 | 0.29 |
| $n=6$ | $[-0.48,1.39]$ | 11.71 | 0.09 |
| $n=7$ (exit) | $[-0.44,1.43]$ | 11.68 | 0.03 |

Table 9.1.: Example output from sequential gradient descent (SGD), Algorithm 9.1, applied to the linear regression problem for data of dimension $D=2$, first coordinate fixed to 1 , with input data size $N=20$. Algorithm parameters were learning rate $\alpha=0.08$ and convergence tolerance $\epsilon=0.05$. In this run, the algorithm terminates at stage $n=7$ with result $w^{\star}=[-0.44,1.43]$ and $F\left(w^{\star}\right)=11.68$, which compares favourably to the known optimal solution, $w=[-0.6,1.5]$.

so that the parameter update in the gradient descent step in SGD is, ${ }^{6}$

$$
\begin{equation*}
w_{n+1}=w_{n}-\alpha \sum_{i=1}^{N}\left(w_{n}^{T} x_{i}-y_{i}\right) x_{i} \tag{9.6}
\end{equation*}
$$

Results of a typical run of SGD using this gradient update step is shown in Table 9.1, which terminates after eight iterations.

For linear regression, the error function is actually convex with respect to the parameter vector $w$, so SGD with the correct choice of learning rate parameter, can converge on the globally optimal solution eventually. ${ }^{7}$

9.4. Model complexity and generalization

The linear regression model investigated above is particularly simple, but for most practical ML problems linear modeling is too simple. More complex and sophisticated regression models include polynomials in the form of sums of powers of the input variables, kernel methods, or neural networks (which we will study in later sections). It is usually the case that a sufficiently complex model, can actually achieve zero training data error, that is, it can obtain $F\left(w^{\star}\right)=0$ at the optimal $w^{\star}$ on the training data. While this may seem like a desirable property for a trained ML model, it is often the case that an algorithm which has very low training set error, will, paradoxically, have high test data error. The reason for this is simple: the measured data often has two parts to it, a systematic component (we choose the mathematical form of the regression model to match this) and a random component to it (which influences the way we quantify prediction error). ${ }^{8}$ Increasing the model complexity means that the model mostly fits the random component, not the underlying,

[^2]![](https://cdn.mathpix.com/cropped/2024_07_30_e5725121dc82b5a4e3a7g-6.jpg?height=772&width=969&top_left_y=328&top_left_x=521)

Figure 9.2.: Illustrating ML model complexity in regression. A complex polynomial regression model (black line) goes almost exactly through the training data (black dots), and thus has small training error, $F\left(w^{\star}\right)=0.04$. By contrast, a simple linear model (orange) misses most of the black dots and so has much higher training error, $F\left(w^{\star}\right)=0.17$. However, the actual model which generated this data is linear, the non-linear arrangement of the data is just pure randomness. Therefore, the complex polynomial model is too complex for this problem, even though the model training error is small.

systematic component (Figure 9.2). So, there is typically a trade off between model complexity and test error, and this is a common feature of most machine learning models. Ideally, we want a model which is as simple as possible, but no simpler. ${ }^{9}$

Learning outcomes for section 9

$\triangleright$ Demonstrate understanding of the principles of machine learning: training versus test data, fitting models, making predictions, error functions, supervised versus unsupervised learning.

$\triangleright$ Be able to apply sequential gradient descent to simple linear regression problems.

\footnotetext{
${ }^{9}$ This principle is often called Occam's razor.


[^0]:    ${ }^{1}$ It is important to note that this bears very little resemblance to the way humans and animals actually learn.

    ${ }^{2}$ Although modern ML algorithms are capable of finding parameters which find the value of $w$ such that $F(w)=0$

    ${ }^{3}$ Note that this is actually a vector of $D$ zeros, but we use the usual zero as a shorthand.

[^1]:    ${ }^{4}$ The vector transpose notation $x^{T}$ turns the row vector $x$ into a column vector, so we can use vector-vector multiplication leading a more compact form of the linear equation.

    ${ }^{5}$ The proof of this is straightforward but takes quite a lot of tedious algebra. Alternatively, we can use standard results from matrix-vector calculus.

[^2]:    ${ }^{6}$ Because we have freedom to choose any learning rate parameter $\alpha^{\prime}$, then we can set $\alpha=2 \alpha^{\prime}$ and eliminate the factor 2 in the gradient.

    ${ }^{7}$ In fact, in the case of linear regression and sum-of-square error, we do not actually need to use SGD, there is a straightforward analytical solution obtained using matrix-vector calculus. Such analytical solutions do not exist for most ML models, however.

    ${ }^{8}$ This is not always true in practice, some modern ML problems and corresponding algorithms are better described as interpolation rather than least-error curve-fitting, in these circumstance smoothness of the model in between data points is a better way to measure model complexity.

Section 10. 

Clustering

Relevant reference reading material for this section is PRML, Section 9.1, MLSP, Section 6.5 and H\&T, Section 13.2.1 and Section 14.3.6.

10.1. Formulating optimal $K$-clustering

A common data analysis problem which arises in practice is clustering. Given $N$ data points in $D$-dimensional data space, e.g. $\mathbb{R}^{D}$, what is the optimal way to partition the set of data into $K$ groups? Clustering is an example of an unsupervised ML problem: we do not have any associated labels. As discussed in Section 3, we can always use an exhaustive combinatorial algorithm, i.e. find all possible ways of partitioning the set of indices $\{1,2, \ldots, N\}$ into $K$ blocks, then find the optimal partition. However, the number of such set partitions is the Stirling numbers of the 2nd kind, for K fixed this number increases like $O\left(K^{N}\right)$ (exponential) so exact combinatorial optimization algorithms are intractable. Therefore, in ML, approximate clustering methods are nearly always used in practice.

To solve the problem computationally, we need to formalize it mathematically. We will treat it as a $K$-clustering problem: partitioning the data into $K$ non-overlapping clusters such that some objective function of this clustering configuration is minimized. We will use indicator function notation $X_{i k}$, to represent a clustering partition as a combinatorial configuration,

$$
X_{i k}= \begin{cases}1 & \text { data item } x_{i} \text { is assigned to cluster } k  \tag{10.1}\\ 0 & \text { otherwise }\end{cases}
$$

As an example, if we have data $x_{1}, \ldots, x_{5}$ and $K=3$ classes, then the configuration,

$$
X=\left[\begin{array}{lll}
0 & 0 & 1  \tag{10.2}\\
0 & 0 & 1 \\
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 1 & 0
\end{array}\right]
$$

implies $x_{1}, x_{2}$ are assigned to cluster $3, x_{3}$ is assigned to cluster 1 and $x_{4}, x_{5}$ are assigned to cluster 2. This form is very convenient mathematically because we can then express the so-called $K$-means objective, a widely-used measure of clustering quality, directly in terms of this indicator function,

$$
\begin{equation*}
F(X, \mu)=\sum_{i=1}^{N} \sum_{k=1}^{K} X_{i k}\left\|x_{i}-\mu_{k}\right\|^{2} \tag{10.3}
\end{equation*}
$$

where in this equation, \|\|$^{2}$ is the square Euclidean norm of a vector, which is just the sum of squares of each dimension,

![](https://cdn.mathpix.com/cropped/2024_07_30_74606bd6d52a196f1278g-2.jpg?height=749&width=941&top_left_y=299&top_left_x=546)

Figure 10.1.: Illustrating the objective function of $K$-means clustering in $D=2$ and a partition of $K=2$ clusters. This shows the measure of clustering quality $F(X, \mu)=\sum_{i=1}^{N} \sum_{k=1}^{K} X_{i k}\left\|x_{i}-\mu_{k}\right\|^{2}$ where the terms $\left\|x_{i}-\mu_{k}\right\|$ are the Euclidean distances between the coordinates of the cluster centroid $\mu_{k}$ (cross) and the coordinates of the data point $x_{i}$ (dot). The colour (black, blue) indicates the assignment of the data point to clusters, given by the partition indicator (configuration) $X_{i k}$.

$$
\begin{equation*}
\|v\|^{2}=\sum_{d=1}^{D}\left(v^{d}\right)^{2} \tag{10.4}
\end{equation*}
$$

The quantity $\mu_{k}$ is the average of the data in each coordinate of each cluster, which is called the centroid for that cluster, which we can also write as,

$$
\mu_{k}=\frac{1}{N_{k}} \sum_{i=1}^{N} X_{i k} x_{i}, \quad N_{k}=\sum_{i=1}^{N} X_{i k}
$$

It is helpful to visualize an example $K$-clustering in $D=2$, showing how the objective is sensitive to the spread of the data around the centroids (Figure 10.1).

The $K$-means clustering problem thus attempts to solve the following optimization problem,

$$
\begin{equation*}
\left(X^{\star}, \mu^{\star}\right)=\underset{\left(X^{\prime}, \mu^{\prime}\right) \in \mathcal{W}}{\arg \min } F\left(X^{\prime}, \mu^{\prime}\right) \tag{10.5}
\end{equation*}
$$

where $\mathcal{W}$ is the set of all possible partitions (i.e. just those indicators where each data item is assigned to a unique class), along with their corresponding centroid averages.

10.2. The $K$-means algorithm

Note that, given fixed clustering configuration $X$, we can always find the globally optimal corresponding centroids $\mu$, and given fixed $\mu$, we can determine the globally optimal corresponding configuration


Algorithm 10.1 The $K$-means algorithm for approximate clustering.

$\triangleright$ Step 1. Initialization: Start with $n=0$ and an initial guess for all centroids $\mu_{k}^{0}$ for $k=$ $1,2, \ldots, K$,

$\triangleright$ Step 2. Update configuration: Set all entries in $X^{n+1}=0$, except where $k=$ $\arg \min _{k^{\prime}=1,2, \ldots, K}\left\|x_{i}-\mu_{k^{\prime}}\right\|^{2}$, for which $X_{i k}^{n+1}=1$,

$\triangleright$ Step 3. Update centroids: Compute $\mu_{k}^{n+1}=\frac{1}{N_{k}} \sum_{i=1}^{N} X_{i k}^{n+1} x_{i}$ where $N_{k}=\sum_{i=1}^{N} X_{i k}^{n+1}$,

$\triangleright$ Step 4. Convergence check: If $n>0$ and $X^{n+1}=X^{n}$, then exit with solution $X^{\star}=X^{n+1}$ and $\mu^{\star}=\mu^{n+1}$,

$\triangleright$ Step 5. Iteration: Update $n=n+1$ and go back to step 2 .

$X$, so they actually contain the same information. ${ }^{1}$ This suggests an idea: start with a good guess for each cluster centroid $\mu_{k}$, then use these guesses to compute the optimal corresponding partition entries in $X_{i k}$; now use that to get a better guess for all the $\mu_{k}$, and so on. The resulting algorithm is known as $K$-means clustering (Algorithm (10.1)).

We can show that $F\left(X^{n+1}, \mu^{n+1}\right) \leq F\left(X^{n}, \mu^{n}\right)$ the $K$-means objective (10.3) is never increasing (because given $\mu$ we can find the X with globally smallest $F$, and vice-versa). So, the $K$-means algorithm always converges on a fixed point (that is, it finds a minima of $F$ and stays there for all subsequent iterations). But, we cannot know if the minima is local or global, furthermore we cannot know in advance how many iterations it takes to converge (typically $7-15$ iterations from experience, but there are no guarantees about this). Another limitation of the algorithm is the potential for pathological cases, for instance, in updating configurations (step 2), it is perfectly possible for a cluster to have no data items assigned to it; then it is impossible to find the centroid for that cluster on the next step. ${ }^{2}$

Because of the exponential number of ways of partitioning a set into $K$ clusters, most initial guesses for the configuration $X$ can be substantially improved upon. Figure 10.2 shows the typical evolution of the $K$-means objective function during a run of the $K$-means algorithm, showing how the improvement in objective is usually fast at the start and then slows down at convergence. Convergence usually takes only a handful of iterations in practice.

10.3. Example: patch-based digital image dictionaries

Here is an interesting problem in wildlife conservation where a clustering problem arises. Remote, battery-operated digital cameras are set up in the wild to take images of wildlife for ecological purposes such as tracking and counting. Because the cameras are expected to operate entirely remotely for many months at a time, they have to be extremely energy-efficient. This restricts the amount of computational processing they can have on board (fast, capable processors are energy-hungry). All the digital images have to be analyzed to determine whether they should be stored or discarded since memory is limited on the hardware. However, each digital image contains a lot of information. For instance, a single image of around 300 by 600 pixels requires 600 kb and the camera take an image

[^0]![](https://cdn.mathpix.com/cropped/2024_07_30_74606bd6d52a196f1278g-4.jpg?height=660&width=1286&top_left_y=321&top_left_x=379)

Figure 10.2.: The typical evolution of the $K$-means objective function value $F\left(X^{n}, \mu^{n}\right)$ during iteration of the $K$-means algorithm (Algorithm 10.1).

every minute, so 1440 images per day means processing close to 1 Gb per day. One way to represent these images more efficiently is to simplify them by replacing every small patch of pixels in the image, with a set of patches in a limited dictionary of pixel patches. Then, the only information needed to represent an image, is the patches in the dictionary, and for every patch in the image, the associated patch in the dictionary, which can then be used to reconstruct the image. If the dictionary contains sufficiently diverse patches, we expect that not much will be lost by simplifying the images this way.

$K$-means is applied to the input greyscale (black and white) JPG image of size 328 (vertical) by 640 (horizontal) pixels, with one byte grey scale per pixel (Figure 10.3). This gives an image data size of 209920 bytes. The $K$-means model has $K=256$ patches of size $4 \times 4=16$ pixels, so that every cluster centroid $\mu_{k}$ is a $4 \times 4$ greyscale patch. So, the $K$-means model has $16 \times 256=4096$ bytes. The single byte cluster indices, one per patch in the original image, map patches to clusters. There are $328 / 4=82$ (vertical) by $640 / 4=160$ (horizontal) i.e. 13120 such patches. Therefore, the total amount of space required to store the image in terms of the $K$-means model is $4096+13120=17216$ bytes. This represents a ratio of about $12: 1$ i.e. the image is compressed) into only $1 / 12$ th of the space of the original. This leads to a substantial saving in computational resources required to process the data.

Image dictionary-based methods such as this perform a similar function to more advanced techniques such as auto-encoders used in cutting-edge vision-language AI to find patches in images with the same or similar 'meaning' across multiple millions of images, paired with descriptive text. For instance, some of the $K$-means patches in this image occur much more frequently than the rest, these are associated with visual features such as the sky and grass. On a large enough scale, this kind of statistical association can be used to accurately predict the most likely association of text strings with image patches, for instance, to automatically complete fairly complex queries like "is there more than one wildebeest in this image"?

Figure 10.3.: Using $K$-means clustering to compress greyscale images for resource sensitive remote sensing applications in conservation. The top panel shows the original image, the bottom panel the image reconstructed from a $K$-means model with $K=256$ clusters representing patches of $4 \times 4$ pixels.

Learning outcomes for section 10

$\triangleright$ Demonstrate understanding of clustering as an unsupervised machine learning problem: partitioning, indicator functions, centroids, $K$-means clustering objective.

$\triangleright$ Explain the steps in the $K$-means clustering algorithm and their significance for optimizing the clustering objective.

$\triangleright$ Apply the $K$-means clustering algorithm to small data clustering problems.


[^0]:    ${ }^{1}$ To see this, note that (10.3) is possible to optimize globally, using calculus, with respect to one of either $X$ or $\mu$ if the other is fixed. When neither is fixed the problem is intractable, which is of course, the main justification for the $K$-means algorithm.

    ${ }^{2}$ Combinatorially, this occurs because we have not excluded partitions which can have empty sets.

Section 11. 

Classification

Reference reference reading material relevant to this section is $\mathbf{R} \& \mathbf{N}$, Section 18.2, PRML, Section 2.5 and $\mathbf{H \& T}$, Section 13.3.

11.1. Partitioning feature space based on labeled training data

A classification ML model learns to split the given training data into two or more classes, according to the training data. The hypothesis is that there is some decision (classification) boundary in the data which makes this classification possible. As with regression, it is supervised: given input training data and corresponding labeled output training data, the a training algorithm attempts to find the decision boundary implied by a classification model $f(w, x)$. The value of the parameters $w$ determines a specific decision boundary out of all possible boundaries.

As an example of classification, consider the medical problem of creating an ML algorithm to determine whether someone is likely to have a voice pathology or not and if so, pass them on to further medical investigation ${ }^{1}$ (Figure 11.1). We are given some training data $\left(x_{i}, y_{i}\right)$ for $i=1,2, \ldots, N$ individuals, which consists of (e.g. in $D=2$ ) a pair of diagnostic feature data, for instance, two measurable aspects of voice quality, $x^{1}$ and $x^{2}$, and a suspected diagnosis of pathology $(y=+1)$ or not $(y=-1)$. Using the classification model $f\left(w^{\star}, x\right)$ where $w^{\star}$ are a good set of parameters, if this can be made to accurately predict the labels $y_{i}$ given the $x_{i}$ as input so $f\left(w^{\star}, x_{i}\right) \approx y_{i}$, then we would have confidence that the classification is reliable for test data and possibly for use in medical practice. We do not expect that the classifier is perfect, but we want to make it accurate so as to avoid clinical errors. In medical terms, we want the automated classification algorithm to minimize the number of false positives (incorrectly triaging someone healthy) and false negatives (missing a pathological case).

11.2. The perfect classifier?

Is it possible to construct an ML classifier which never makes any mistakes? Here is a simple idea. Assume that the model $f(w, x)$ acts like a perfect look-up table which contains a list of every possible value of the input $x$ and its associated output $y=f(x, w)$. It is easy to see that, for any dataset, this will always make zero classification errors. This seems like a good idea so let us try to make it work for a practical ML example, automatically transcribing handwritten messages. First we need to gather all possible input images of letters. To do this, we first need to plan how much memory we need for the big the look-up table. Take a typical image of $16 \times 16=256$ pixels, each of which is greyscale 8 bits. This means $2^{16 \times 16 \times 8}$ possible images, which works out at approximately $10^{616}$ images! Unfortunately, this is completely impractical: even if we could somehow capture images of all possible images, there is no database in the universe large enough to even store them.

\footnotetext{
${ }^{1}$ In medical terms this kind of activity is called triage.

![](https://cdn.mathpix.com/cropped/2024_07_30_af4f9252b215251d0eb0g-2.jpg?height=781&width=1009&top_left_y=295&top_left_x=518)

Figure 11.1.: Classification applied to medical triage, illustrating the main concepts in ML classification. A classification model $f(x, w)$ is trained on this the feature data (here, two medical diagnostic features from a set of individuals, $x^{1}, x^{2}$ ) to find the best parameters $w^{\star}$ which optimally split the space of the feature data into two regions, one each for class -1 (healthy) and class +1 (pathology). The classification (decision) boundary, determined by the classifier $f\left(x, w^{\star}\right)$, splits the feature space into these two regions.

This illustrates a critical point about ML classifiers (and in fact, all supervised models). In principle, any supervised machine learning problem can be completely solved by storing a table of all possible input-output pairs, then prediction on test data is just table look-up. But in practice, all useful ML classifiers are imperfect models. The conclusion is that ML is more than just memorization, it requires careful (mathematical and computational) modeling.

Modern large-scale deep learning classification algorithms trained on extremely large datasets, often behave as if they are merely memorizing the training data, but this cannot literally be true as they are capable of producing outputs for any range of inputs. Complex deep learning algorithms are closer to memorization with some modeling. At the other extreme, linear classification models are very simple so they are nearly all modeling and little to no memorization. As with regression, there is usually a trade-off between increasing model complexity and reducing test error. Many complex classifiers can be made to fit the training data with almost exactly zero classification error. But such a classifier may not work very well at all on test data due to the effect of randomness in how the data arises in the real world. Overall, we prefer the simplest model evidenced by the data, but no simpler than that (Figure 11.2). ${ }^{2}$

11.3. Classification defined mathematically

We can state the goal of classification as one of minimizing the misclassification error (the sum of incorrectly classified data points in the training data set) of the model $f(w, x)$ on the training data

[^0]![](https://cdn.mathpix.com/cropped/2024_07_30_af4f9252b215251d0eb0g-3.jpg?height=697&width=872&top_left_y=294&top_left_x=586)

Figure 11.2.: A trade-off usually exists between classifier complexity (reflected in the 'wigglyness of the decision boundary), and prediction accuracy on the test data. A complex classifier can achieve low training error by detailed routing of the decision boundary around the training data, but this is not a good decision boundary if randomness is ignored.

pairs $\left(x_{i}, y_{i}\right)$,

$$
\begin{equation*}
F(w)=\sum_{i=1}^{N} \mathbb{I}\left[f\left(w, x_{i}\right) \neq y_{i}\right] \tag{11.1}
\end{equation*}
$$

where $\mathbb{I}$ [] is the indicator function given by, ${ }^{3}$

$$
\mathbb{I}[P]= \begin{cases}1 & \text { if logical condition } P \text { is true }  \tag{11.2}\\ 0 & \text { otherwise }\end{cases}
$$

This indicator function form of the classification error function, is the reason why (11.1) is also sometimes called the $\mathbf{0 - 1}$ loss. So, the best classifier for some given training data, is the one which minimizes (11.1),

$$
\begin{equation*}
w^{\star}=\underset{w^{\prime} \in \mathcal{W}}{\arg \min }\left(\sum_{i=1}^{N} \mathbb{I}\left[f\left(w^{\prime}, x_{i}\right) \neq y_{i}\right]\right) \tag{11.3}
\end{equation*}
$$

This optimization problem, while easy to write down, is extremely mathematically challenging to solve. For instance, we might be tempted to use SGD (Algorithm (9.1)) but the the gradient of 1[] is either zero (since the function is flat for nearly every input), or it is not defined (in fact, infinite, so we cannot even calculate its value!) In practice then, most classification algorithms do not use the misclassification error function, they use a proxy or surrogate error (loss) which is much easier to optimize. A surrogate loss we will study in detail in the next section is the so-called perceptron loss,

$$
\begin{equation*}
F(w)=\sum_{i=1}^{N} \max \left[0,-y_{i} f\left(w, x_{i}\right)\right] \tag{11.4}
\end{equation*}
$$

\footnotetext{
${ }^{3}$ The partition indicator in the clustering section can be considered an example of such a function.
which has much 'nicer' behaviour from a computational perspective. Other surrogate losses, such as the hinge loss and logistic loss have additional desirable properties and are widely used in practice. Nonetheless, it is important to remember that the problem of optimal classification is only correctly solved by optimizing (11.1) in general, no matter how mathematically convenient a surrogate might be.

Learning outcomes for section 11

$\triangleright$ Explain the principles of supervised classification, including classifier model training, decision boundaries, the contrast between memorization and generalization, 0-1 misclassification error.

$\Delta$ Demonstrate understanding of the form and importance of surrogate losses, in particular the perceptron loss.


[^0]:    ${ }^{2}$ This does mean that linear models can be (and indeed often are) too simple in practice for most real-world ML applications.


Section 12. 

The perceptron

Relevant reference reading material for this section is PRML, Section 4.1.7, R\&N, Section 18.6.3 and $\mathbf{H} \& \mathbf{T}$, Section 4.5.1.

12.1. Linear classification with perceptron loss

The perceptron is a simple linear classifier based on a surrogate for the $0-1$ loss, the perceptron loss. It is a very important classical algorithm in the history of ML, a direct precursor to modern deep learning algorithms. Because it is extremely simple there are mathematically much better linear algorithms, for instance, support vector machines, so the perceptron is rarely used in practice today. Nonetheless, understanding how this classifier works is critical to understanding most of the main principles of modern ML classification.

To see how the perceptron arises, note that we can easily change a linear regression model, $f(w, x)=$ $w^{T} x$, into a classification model by choosing $f(w, x)=\operatorname{sign}\left(w^{T} x\right)$. This function is +1 if $w^{T} x>0$ and -1 if $w^{T} x<0 .{ }^{1}$ The decision boundary for this classifier occurs where $w^{T} x=0$. For this classifier, the associated misclassification $(0-1)$ error would be $F(w)=\sum_{i=1}^{N} \mathbb{I}\left[\operatorname{sign}\left(w^{T} x_{i}\right) \neq y_{i}\right]$. However, as discussed in previous section, the problem with this $0-1$ loss error function is that it is not differentiable, so we cannot find good values of the parameters $w$ by training using gradient descent. Instead, we will use the more convenient perceptron objective function,

$$
\begin{equation*}
F(w)=\sum_{i=1}^{N} \max \left(0,-y_{i} w^{T} x_{i}\right) \tag{12.1}
\end{equation*}
$$

To understand this error function, it helps to focus on the loss expression, $\max \left(0,-y w^{T} x\right)$ (Table 12.1). There are four different combinations of cases of the sign of the linear model and the true label $y$; two cases correspond to correct decisions by the model, and two incorrect. When the linear model correctly classifies, the perceptron loss is 0 as with the $0-1$ loss. However, if the linear model incorrectly classifies, then the perceptron loss is $w^{T} x$, which is just the distance to the decision boundary in the direction $w$. So, the perceptron error 'penalizes' incorrect decisions by the model, to the extent to which they are incorrect, and the perceptron error is the sum of all penalties for the whole data set. This contrasts with the misclassification error, which only counts the number of incorrect decisions in the data set (Figure 12.1).

12.2. SGD for classification: the perceptron algorithm

While the perceptron error does not actually measure the number of misclassified points (because it is a surrogate error), it is differentiable with respect to the linear parameters $w$. We can therefore apply

\footnotetext{
${ }^{1}$ The case $w^{T} x=0$ leads to $\operatorname{sign}\left(w^{T} x\right)=0$ which means $x$ lies exactly on the decision boundary.

| Linear <br> regression <br> $w^{T} x$ | Linear <br> classifier <br> $\operatorname{sign}\left(w^{T} x\right)$ | True <br> label $y$ | $-y w^{T} x$ | Classifier <br> decision | Perceptron <br> loss <br> $\max \left(0,-y w^{T} x\right)$ | $\mathbf{0}[\mathbf{1}$ loss <br> $\left.\mathbb{i} \operatorname{sign}\left(w^{T} x\right) \neq y\right]$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $>0$ | +1 | +1 | $-w^{T} x$ | Correct | 0 | 0 |
| $<0$ | -1 | -1 | $w^{T} x$ | Correct | 0 | 0 |
| $<0$ | -1 | +1 | $-w^{T} x$ | Incorrect | $w^{T} x$ | 1 |
| $>0$ | +1 | -1 | $w^{T} x$ | Incorrect | $w^{T} x$ | 1 |

Table 12.1.: Understanding the perceptron loss function, $\max \left(0,-y w^{T} x\right)$. This function 'penalizes' incorrect decisions by the linear classifier, by the distance from the decision boundary $w^{T} x$ in the direction $w$. This compares to the misclassification (0-1) loss function which assigns the same penalty to all incorrect decisions, regardless of how 'bad' they are.

![](https://cdn.mathpix.com/cropped/2024_07_30_f6a0a1b1cf3b4fec7b0bg-2.jpg?height=889&width=1466&top_left_y=1189&top_left_x=295)

Figure 12.1.: Illustrating the linear perceptron classifier in $D=2$ feature space with coordinates $x^{1}$ and $x^{2}$. Data points with label $y=+1$ are black dots, $y=-1$ blue dots. The linear classifier model is $f(w, x)=\operatorname{sign}\left(w^{T} x\right)$ which implies a decision boundary $w^{T} x=0$ (orange line). The perceptron error, $F(w)=\sum_{i=1}^{N} \max \left(0,-y_{i} w^{T} x_{i}\right)$ is the sum of the perpendicular distances (lines with arrowheads) of every misclassified data point to the decision boundary. Correctly classified points contribute 0 to the error (as with the misclassification error).


Algorithm 12.1 The perceptron (training) algorithm.

$\triangleright$Step 1. Initialization: Select a starting candidate classification model $w_{0}$, set iteration number $n=0$, choose maximum number of iterations $R$ and learning rate $\alpha>0$

$\triangleright$ Step 2. Gradient descent step: Compute new model parameters. Set $w_{n+1}=w_{n}$, and taking each $i=1,2, \ldots, N$ in turn, if $\operatorname{sign}\left(w_{n}^{T} x_{i}\right) \neq y_{i}$, then $w_{n+1}=w_{n+1}+\alpha y_{i} x_{i}$,

$\triangleright$ Step 3. Iteration: If $n<R$, set $n=n+1$, go back to step 2 , else exit with solution $w^{\star}=w_{n}$.


SGD (Algorithm 9.1) to training the perceptron classifier. To do this, we need the gradient of (12.1), which is, ${ }^{2}$

$$
\begin{equation*}
F_{w}(w)=-\sum_{i=1}^{N} y_{i} x_{i} \mathbb{I}\left[-y_{i} w^{T} x_{i} \geq 0\right] \tag{12.2}
\end{equation*}
$$

so that the parameter update in the gradient desent step in SGD is,

$$
\begin{equation*}
w_{n+1}=w_{n}+\alpha \sum_{i=1}^{N} y_{i} x_{i} \mathbb{I}\left[-y_{i} w^{T} x_{i} \geq 0\right] \tag{12.3}
\end{equation*}
$$

We can express this in a simple, 'intuitive' form: first setting $w_{n+1}=w_{n}$ then for all $i=1,2, \ldots, N$, if $\operatorname{sign}\left(w^{T} x_{i}\right) \neq y_{i}$, then $w_{n+1}=w_{n+1}+\alpha y_{i} x_{i} .{ }^{3}$ This is the form we used to describe the perceptron (training) algorithm (Algorithm 12.1).

Results of a typical run of the perceptron algorithm given in Table 12.2. This shows that, unlike with SGD applied to linear regression, the perceptron objective function does not always decrease on each iteration: the only situation where the objective function does not increase is where the data is linearly separable, that is, it can be perfectly separated into two classes by a linear classifier.

Analysis of this algorithm shows that, it will converge on the perfect boundary (i.e. the one which minimizes the misclassification error) if the data is linearly separable. Otherwise, the algorithm may fail to converge (never settles down on a single solution) and can become chaotic. This happens because the algorithm is jumping between different local minima of the objective function. This is the reason why we choose to exit after a fixed number of iterations rather than test for convergence as in the description of the original SGD algorithm.

[^0]
| Iteration | Parameters $w_{n}$ | Error function <br> value $F\left(w_{n}\right)$ | Error function <br> gradient $F_{w}\left(w_{n}\right)$ |
| :--- | :--- | ---: | :--- |
| $n=0$ (initialize) | $[-0.94,0.34]$ | 4.59 | $[-5.63,-2.14]$ |
| $n=1$ | $[-0.66,0.44]$ | 2.77 | $[-5.63,-2.14]$ |
| $n=2$ | $[-0.38,0.55]$ | 0.96 | $[-5.63,-2.14]$ |
| $n=3$ | $[-0.10,0.66]$ | 0.56 | $[-5.63,-2.14]$ |
| $n=4$ | $[-0.18,0.60]$ | 0.43 | $[1.60,1.09]$ |
| $n=5$ | $[-0.14,0.58]$ | 0.41 | $[-0.66,0.53]$ |
| $n=6$ | $[-0.20,0.53]$ | 0.44 | $[1.10,0.99]$ |
| $n=7$ | $[-0.12,0.51]$ | 0.38 | $[-1.54,0.25]$ |
| $n=8$ | $[-0.18,0.46]$ | 0.39 | $[1.10,0.99]$ |
| $n=9$ (exit) | $[-0.10,0.45]$ | 0.34 | $[-1.54,0.25]$ |

Table 12.2.: Example output from the perceptron algorithm (Algorithm 12.1) for data of dimension $D=2$, with input data size $N=20$, data shown in Figure 12.2. This data is not linearly separable. Algorithm parameters: learning rate $\alpha=0.05$ and maximum number of iterations, $R=10$. In this run, the algorithm terminates with result $w^{\star}=[-0.10,0.45]$ and $F\left(w^{\star}\right)=0.34$.

![](https://cdn.mathpix.com/cropped/2024_07_30_f6a0a1b1cf3b4fec7b0bg-4.jpg?height=618&width=1368&top_left_y=1553&top_left_x=344)

Figure 12.2.: Example evolution of the perceptron algorithm (Algorithm 12.1) for data of dimension $D=2$, with input data size $N=20$. This data is not linearly separable. Algorithm parameters: learning rate $\alpha=0.05$ and maximum number of iterations, $R=10$. In this run, the algorithm terminates with result $w^{\star}=[-0.10,0.45]$ and $F\left(w^{\star}\right)=0.34$.

Learning outcomes for section 12

$\triangleright$ Demonstrate understanding of the perceptron classifier: linear classification modelling, function and geometric meaning of the perceptron objective function.

$\triangleright$ Have knowledge of and be able to explain the steps in the perceptron training algorithm.

$\triangleright$ Apply the perceptron training algorithm to small classification problems.


[^0]:    ${ }^{2}$ Can be derived using basic properties of calculus but we postpone this until the section on automatic differentiation.

    ${ }^{3}$ To see why this arises, note that $1\left[-y_{i} w^{T} x_{i} \geq 0\right]=1\left[\operatorname{sign}\left(w^{T} x_{i}\right) \neq y_{i}\right]$ and then expand the indicator function 1[] .

Section 13. 

Neural networks and deep learning

Relevent reference reading material for this section: PRML, Section 5.1 and H\&T, Section 11.3.

13.1. Activation nonlinearities

So far, we have only encountered the ReLU activation function arising in the perceptron. Nonlinear functions used in deep networks can, in principle be any nonlinear function, but to be useful in practice they ought to be easy to use in calculations, for instance, to be easy to compute and simple to differentiate (Table 13.1 and Figure 13.1). It is a notable feature of activation functions, that often either the function or its gradient is used. By far the most widely used activation function it the ReLU function, probably because of its simplicity: for half of its range it is exactly zero which simplifies computations in complex deep nets substantially.

13.2. Deep neural networks: chained perceptrons

We can view the simple linear perceptron model $f(w, x)=\max \left(0, w^{T} x\right)$ in the form of a weighted linear combination with nonlinear activation function,

$$
\begin{equation*}
y=\max \left(0, w^{T} x\right) \tag{13.1}
\end{equation*}
$$

This interpretation is inspired by the biological neuron, which has axons, dendrites and a nucleus, which are organized so as to 'fire' (produce a non-zero output) when the sum of the inputs crosses a certain threshold level. ${ }^{1}$ The perceptron is very limited and can only model linear decision boundaries. In practical ML applications, more complex nonlinear boundaries are required. To extend the capability of these simple models, we can chain them together so the output of one, feeds into the input of another. For instance, a set of neurons acting on the same inputs $x$, whose outputs $z$ are then

\footnotetext{
${ }^{1}$ This is only a very vague and inaccurate description of how neurons actually work!

| Activation function | Expression | Derivative | Expression |
| :--- | :--- | :--- | :--- |
| ReLU (rectified linear unit) | $\max (0, x)$ | Step function | $\mathbb{I}[x \geq 0]$ |
| Softplus | $\ln \left(1+e^{x}\right)$ | Logistic (sigmoid) | $\frac{1}{1+e^{-x}}$ |
| Hyperbolic tangent | $\tanh (x)$ | Hyperbolic tangent gradient | $1-\tanh (x)^{2}$ |

Table 13.1.: A selection of widely-used nonlinear activation functions in modern deep neural networks, and their corresponding gradient functions.

![](https://cdn.mathpix.com/cropped/2024_07_30_151247482eb284cd9e4eg-2.jpg?height=1072&width=1232&top_left_y=800&top_left_x=412)

Figure 13.1.: Widely encountered nonlinear activation functions used in modern deep neural networks. The column on the right are all derivatives of the corresponding function in the same row on the left.

Figure 13.2.: (Left) The perceptron as a single neuron which takes multiple inputs, sums them together and applies a nonlinear activation (here, the ReLU) function to create the output, $y=\max \left(0, w^{T} x\right)$ (Right) Extending the perceptron to two layers, with nonlinear activations connecting them through hidden neurons $z$ whose output is fed into the output layer creating the final output $y$, e.g. $z=$ $\max \left(0, W^{T} x\right)$ and $y=\max \left(0, v^{T} z\right)$, leads to the basic multi-layer perceptron (MLP). This is the simplest (fully-connected, two-layer) deep learning algorithm.

fed into the input of another neuron, leads to the simple two-layer multilayer perceptron (MLP):

$$
\begin{align*}
& z=\max \left(0, W^{T} x\right) \\
& y=\max \left(0, v^{T} z\right) \tag{13.2}
\end{align*}
$$

where $W$ is a matrix of weights, so that each column is a vector of weights for each of the $M$ hidden perceptrons in the second layer,

$$
W=\left[\begin{array}{cccc}
w_{1,1} & w_{1,2} & \cdots & w_{1, M}  \tag{13.3}\\
w_{2,1} & w_{2,2} & \cdots & w_{2, M} \\
\vdots & \vdots & \ddots & \vdots \\
w_{D, 1} & w_{D, 2} & \cdots & w_{D, M}
\end{array}\right]
$$

and $v_{1}, v_{2}, \ldots, v_{M}$ are the weights for the perceptron at the output layer (Figure 13.2).

Extending to multiple layers in this way, with more than two sets of hidden neurons, leads to the number of total number weights growing rapidly (for example, for three fully connected layers with $D$ inputs, $M$ nodes in the first hidden layer, $N$ in the second layer and a single output node, the number of weights is $D M+M N+N)$. Furthermore, there are situations where it makes sense to have the weights shared between connections. A classical example of this is the convolutional neural network (CNN) which is well-suited to ordered data such as images and time series (Figure 13.3). For instance, in image data, it does not usually matter where exactly in the image an object is located - it could be translated up/down or left/right ${ }^{2}$ - so an object detection neural network might have weight sharing in overlapping patches (windows).

\footnotetext{
${ }^{2}$ This property is known as translation invariance.

![](https://cdn.mathpix.com/cropped/2024_07_30_151247482eb284cd9e4eg-4.jpg?height=521&width=644&top_left_y=328&top_left_x=432)

$$
\begin{gathered}
z^{1}=\max \left(0, w^{T}\left[x^{I} x^{2}\right]^{T}\right) \\
z^{2}=\max \left(0, w^{T}\left[x^{2} x^{3}\right]^{T}\right) \\
\cdots \\
z^{M}=\max \left(0, w^{T}\left[x^{D-1} x^{D}\right]^{T}\right) \\
y=\max \left(0, v^{\mathrm{T}} z\right)
\end{gathered}
$$

Figure 13.3.: A simple convolutional neural network (CNN), which shares two weights in the input layer across the whole input sequence, e.g. $w_{1} x^{1}+w_{2} x^{2}, w_{1} x^{2}+w_{2} x^{3}, w_{1} x^{3}+w_{2} x^{4}$ etc. This allows the network to learn patterns where the specific location in the sequence of the input, does not matter, and is well suited to time series (it is said to have 'time shift invariance').

 13.3. Example: deep logic networks

Multilayer deep networks can compute pretty much any desired function of the inputs. Here we will explore the use of deep networks for a problem in symbolic AI: computing logical functions such as the connectives in propositional calculus (Section 8).

To illustrate the idea, the activation we will use is the function $\operatorname{sign}(x)$ which is +1 if $x>0$, -1 if $x<0$ and 0 otherwise. This is much like the step function in Figure 13.1. The goal will be to use the output of the activation $\operatorname{sign}(x)$ with -1 representing False, and +1 representing True. We will construct a system of logical computation based on the use of the neural network function $f_{b}(w, x)=\operatorname{sign}\left(w_{0}+w_{1} x^{1}+w_{2} x^{2}\right)^{3}$ for the binary operators 'and' and 'or', and for the 'not' operator we will use the single input neural network function $f_{u}(w, x)=\operatorname{sign}\left(w_{0}+w_{1} x^{1}\right)$. For the 'and' function, under this encoding, $w_{\text {and }}=[-1,1,1]$ behaves as required. Similarly, for the 'or' function, weights $w_{\text {or }}=[1,1,1]$ work, and for the 'not' function, $w_{\text {not }}=[0,-1]$ suffices. ${ }^{4}$ So, our single-layer logical operator neurons are given by the following very simple functions,

$$
\begin{align*}
f_{\text {and }}\left(x^{1}, x^{2}\right) & =\operatorname{sign}\left(x^{1}+x^{2}-1\right) \\
f_{\text {or }}\left(x^{1}, x^{2}\right) & =\operatorname{sign}\left(x^{1}+x^{2}+1\right)  \tag{13.4}\\
f_{\text {not }}(x) & =\operatorname{sign}(-x) .
\end{align*}
$$

as is straightforward to check, with reference to the logical truth tables for these operators. We can visualize these as single layer linear neurons (Figure 13.4).

Given these functions, we can now construct any desired logical function, by translating the corresponding logical expression into the corresponding neural network function, which will be an appropriate composite of the three functions (13.4). For instance, the 'xnor' function, which is true if the value of the two inputs agree, is given by the logical expression $y(u, v)=(u \wedge v) \vee(\neg u \wedge \neg v)$, which

[^0]![](https://cdn.mathpix.com/cropped/2024_07_30_151247482eb284cd9e4eg-5.jpg?height=315&width=981&top_left_y=294&top_left_x=549)

Figure 13.4.: Implementing the three fundamental logical connectives, 'and', 'or' or 'not' using singlelayer linear neurons with sign activation function, using the encoding +1 for True and -1 for False..

![](https://cdn.mathpix.com/cropped/2024_07_30_151247482eb284cd9e4eg-5.jpg?height=598&width=738&top_left_y=792&top_left_x=679)

Figure 13.5.: The 'xnor' function $y(u, v)=(u \wedge v) \vee(\neg u \wedge \neg v)$ implemented as a multilayer neural network with two hidden layers, using neurons with sign activation function. The logical encoding is +1 for True and -1 for False. (The constant nodes are essentially not inputs so they have been moved out of the way of the inputs for clarity).

in terms of the neural network functions is, ${ }^{5}$

$$
\begin{equation*}
y(u, v)=f_{\text {or }}\left(f_{\text {and }}(u, v), f_{\text {and }}\left(f_{\text {not }}(u), f_{\text {not }}(v)\right)\right) \tag{13.5}
\end{equation*}
$$

When visualized, we can see that (in this implementation at least) we need two hidden layers (Figure 13.5), the first hidden layer to compute the intermediate terms $\neg u, \neg v$ and $u \wedge v$, and the second to compute the intermediate result $\neg u \wedge \neg v .{ }^{6}$ In fact, the 'xnor' function is an example of a simple logical function which a single layer linear neuron cannot compute, which shows the need for multilayer networks.

[^1]
 Learning outcomes for section 13

$\triangleright$ Demonstrate understanding of the fundamental principles of deep learning: activation nonlinearities, linear transformations, chained perceptrons, fully connected layers.

$\Delta$ Explain the significance and mathematical structure of weight sharing in convolutional networks.

$\triangleright$ Perform computations using simple deep networks.


[^0]:    ${ }^{3}$ This is in the familiar form $w^{T} x$ if $x^{1}=1$ and we index dimensions starting at 0 rather than 1.

    ${ }^{4}$ These weights were determined by solving a system of (overdetermined) linear equations for each operator. In fact, this can be done for any reasonable activation function.

[^1]:    ${ }^{5}$ This is the disjunctive normal form for the 'xnor' function

    ${ }^{6}$ There is a way to simplify this to remove one hidden layer if we use the fact that $\operatorname{sign}(x)=x$, provided the inputs are always logical values -1 and +1 .


 Section 14. 

 Automatic differentiation

Relevant reference reading material for this section: PRML, Section 5.3, R\&N, Section 18.7 and $\mathbf{H \& T}$, Section 11.4 and Section 11.7.

 14.1. Deep neural networks and automatic differentiation

Gradient descent is an extremely versatile method for producing good solutions to optimization problems in ML. Single layer linear perceptron training was shown to be relatively straightforward: just use SGD to minimize the perceptron objective $F(w)$ with respect to the weights $w$, given the input training input data $x$ and labels $y$. The expression (12.2) for the gradient $F_{w}(w)$ is not too complicated to calculate by hand. However, to train hidden layer weights $v$ in deep networks, we must propagate gradients from the output error $F(w)$, all the way through each layer. Analytical gradient expressions quickly become intractable. Noting that we do not generally need to care about the specific form of this gradient function, only that it can be easily calculated, motivates the use of more sophisticated gradient calculation methods. Widely used are backpropagation and automatic differentiation (AD) There are many different backpropagation and AD schemes, principally forward and reverse mode; depending upon whether the gradient computations start with values at the input to the network, or start with values obtained at the output instead. We will discuss forward mode AD for its conceptual simplicity. ${ }^{1}$

Automatic differentiation is a meta-programming approach to gradient calculation. By this it is meant: the actual primitive computations which are carried out by a computer to evaluate some ML function, are 'shadowed' by simultaneous calculation of the gradient of that function in the background (Figure 14.1). ${ }^{2}$ Unlike numerical gradients, AD computations are exact in the sense that, if the gradient functions of the primitives are accurate, then the final gradients will generally be accurate too. Unlike (analytical) symbolic differentiation, the full mathematical expression for the gradient is rarely, if ever, actually calculated; the goal is to efficiently compute accurate gradients the point evaluated by the function that is to be differentiated, rather than anything other aims.

Software packages such as PyTorch and JAX help the programmer largely avoid the need for finding any hand-computed gradients in this way and have become an essential tool in deep learning research and AI industries. In this section we will study how the technique works and carry out some toy calculations to demonstrate this; in practice, gradient computations involve substantial computations and are therefore best explored computationally.

[^0]![](https://cdn.mathpix.com/cropped/2024_07_30_62bfeca903b48d2ca0efg-2.jpg?height=947&width=846&top_left_y=246&top_left_x=659)

Figure 14.1.: Automatic differentiation (AD) obtains the gradients of the output of a deep neural network simultaneous with the output of the network itself, using the information available to, or computed by, the network at each stage.

 14.2. Chain rule of calculus and dual numbers

Nearly all functions computing anything signficantly useful in ML, involve composition, that is, they require chains of functions applied to the output of other functions. Gradients of composite functions such as $F(f(x))$, are found using the chain rule of calculus:

$$
\begin{equation*}
\frac{d}{d x} F(f(x))=F^{\prime}(f(x)) \times f^{\prime}(x) \tag{14.1}
\end{equation*}
$$

where $f^{\prime}(u)$ is shorthand for $\frac{d f}{d u}(u)$. As can be seen, this involves the gradients of the first function in the composite, $f^{\prime}(x)$ and the second function $F^{\prime}(u)$, along with the value of the first function, $f(x)$. The output of the composite $F(f(x))$, will generally become the first function in the next composite, and so on. Therefore, to calculate the gradient of a composite function, the value of the first function is also needed, which implies that for computational efficiency reasons it makes sense to compute the gradient along with the value of the function at the same time. Dual numbers, represented as a pair $\left(u, u^{\prime}\right)$, are a convenient algebraic tool to handle such chained computations, which keeps track of both the current computation's value $u$ and the value of its' derivative $u^{\prime}$ together, as a sequence of functions, $f$, are applied, ${ }^{3}$

$$
\begin{equation*}
f\left(\left(u, u^{\prime}\right)\right)=\left(f(u), f^{\prime}(u) u^{\prime}\right) \tag{14.2}
\end{equation*}
$$

[^1]As long as the gradient of the current transformation $f$ in the chain is available analytically, it is possible to compute the composite of any number of functions in this way. Indeed, most forms of computational operations used in ML are special cases of this rule. Here are some which are widely encountered in, for example, deep neural networks: ${ }^{4}$

$\triangleright$Addition $f(u, v)=u+v$

$$
\begin{equation*}
\left(u, u^{\prime}\right)+\left(v, v^{\prime}\right)=\left(u+v, u^{\prime}+v^{\prime}\right) \tag{14.3}
\end{equation*}
$$

$\triangleright$ Multiplication $f(u, v)=u v$,

$$
\begin{equation*}
\left(u, u^{\prime}\right) \times\left(v, v^{\prime}\right)=\left(u v, u^{\prime} v+v^{\prime} u\right) \tag{14.4}
\end{equation*}
$$

$\triangleright \operatorname{Maximum} f(u, v)=\max (u, v)$,

$$
\begin{equation*}
\max \left(\left(u, u^{\prime}\right),\left(v, v^{\prime}\right)\right)=\left(\max (u, v), u^{\prime} \mathbb{I}[u>v]+v^{\prime} \mathbb{I}[u \leq v]\right) \tag{14.5}
\end{equation*}
$$

$\triangleright$ ReLU activation nonlinearity $f(u)=\operatorname{relu}(u)$,

$$
\begin{equation*}
\operatorname{relu}\left(\left(u, u^{\prime}\right)\right)=\left(\max (0, u), u^{\prime} \mathbb{I}[u \geq 0]\right) \tag{14.6}
\end{equation*}
$$

$\triangleright$ Constants $f(u)=c$,

$$
f\left(\left(u, u^{\prime}\right)\right)=(c, 0)
$$

reflecting the fact that this function is (by definition of being constant) independent of the differentiation variable and therefore has zero gradient.

$\triangleright$ Variables $f(u)=u$,

$$
f\left(\left(u, u^{\prime}\right)\right)=(u, 1)
$$

such as variable parameters in a neural network, since by definition $\frac{d u}{d u}=1$.

Here is an example of a chained calculation carried out using dual numbers. Given the constants $y=3$ and $z=-1$ and variable $x=2$, compute $u(x, y, z)=\max (y z, y+2 x)$ and its derivative, $u_{x}(x, y, z)$. Applying the rules above successively (and using additional symbols for intermediate computational results),

$$
\begin{align*}
\bar{x} & =(2,1) \\
\bar{y} & =(3,0) \\
\bar{z} & =(-1,0) \\
c & =(2,0)  \tag{14.7}\\
c \bar{x} & =(2,0) \times(2,1)=(4,2) \\
r_{1} & =\bar{y} \times \bar{z}=(3,0) \times(-1,0)=(-3,0) \\
r_{2} & =\bar{y}+c \bar{x}=(3,0)+(4,2)=(7,2) \\
\bar{u} & =\max ((-3,0),(7,2))=(7,2),
\end{align*}
$$

therefore $u(x, y, z)=7$ and $u_{x}(x, y, z)=2$. While it is, of course, always possible to find the symbolic derivative of the function $u(x, y, z)$, AD enables entirely 'mechanical' calculational steps which lends itself to software implementation.

\footnotetext{
${ }^{4}$ Some of these are just familiar rules from calculus, such as the sum and product rules.

|  | Dual number values $\left(u, u^{\prime}=\frac{d u}{d w_{1,1}}\right)$ |
| ---: | :--- |
| Inputs | $\bar{x}_{1}=(1.89,0), \bar{x}_{2}=(-2.94,0)$ |
| Linear layer weights | $\bar{w}_{1,1}=(0.98,1), \bar{w}_{2,1}=(-0.27,0), \bar{w}_{1,2}=(-0.55,0), \bar{w}_{2,2}=(-0.10,0)$ |
| Hidden node 1 input | $\bar{w}_{1,1} \bar{x}^{1}+\bar{w}_{2,1} \bar{x}^{2}=(0.98,1) \times(1.89,0)+(-0.27,0) \times(-2.94,0)=(2.63,1.89)$ |
| Hidden node 2 input | $\bar{w}_{1,2} \bar{x}^{1}+\bar{w}_{2,2} \bar{x}^{2}=(-0.55,0) \times(1.89,0)+(-0.10,0) \times(-2.94,0)=(-0.75,0)$ |
| Hidden node 1 output | $\bar{z}^{1}=\max ((0,0),(2.63,1.89))=(2.63,1.89)$ |
| Hidden node 2 output | $\bar{z}^{2}=\max ((0,0),(-0.75,0))=(0,0)$ |
| Linear layer weights | $\bar{v}_{1}=(-1.38,0), \bar{v}_{2}=(-0.73,0)$ |
| Output node input | $\bar{v}_{1} \bar{z}^{1}+\bar{v}_{2} \bar{z}^{2}=(-1.38,0) \times(2.63,1.89)+(-0.73,0) \times(0,0)=(-3.63,-2.60)$ |
| Label | $\bar{y}=(1,0)$ |
| Output | $F\left(w_{1,1}\right)=\max ((0,0),(-1,0) \times(1,0) \times(-3.63,-2.60))=(3.63,2.60)$ |

Table 14.1.: Example automatic differentiation (AD) computations using dual numbers, of the output of a two-layer multilayer perceptron with ReLU activation and perceptron loss.

 14.3. Example: AD for multilayer perceptrons

AD applied to deep learning is straightforward. For example, the simple two layer network with two hidden nodes and $D=2$ dimensional input with ReLU activations and perceptron loss, has the forward computation,

$$
\begin{align*}
z^{1} & =\operatorname{relu}\left(w_{1,1} x^{1}+w_{2,1} x^{2}\right) \\
z^{2} & =\operatorname{relu}\left(w_{1,2} x^{1}+w_{2,2} x^{2}\right)  \tag{14.8}\\
F\left(w_{1,1}\right) & =\operatorname{relu}\left(-y \times\left(v_{1} z^{1}+v_{2} z^{2}\right)\right)
\end{align*}
$$

The resulting computations are shown in Table 14.1. Gradients of the output with respect to the rest of the parameters, e.g. $F\left(w_{1,2}\right)$ and $F\left(v_{1}\right)$ etc. are computed similarly; in practice all gradients are computed in parallel using vector arithmetic.

 Learning outcomes for section 14

$\triangleright$ Be able to explain the purpose of automatic differentiation, its importance for deep learning, and how it differs from numerical and symbolic differentation.

$\triangleright$ Explain and demonstrate how automatic differentiation can be computed using dual numbers.

$\triangleright$ Perform simple gradient computations using dual numbers.


[^0]:    ${ }^{1}$ There are different computational trade-offs between AD schemes, even though all schemes should, in principal, compute the exact same value of the gradient.

    ${ }^{2}$ Some ML researchers call AD differentiable computing for this reason.

[^1]:    ${ }^{3}$ For those familiar with complex numbers, dual numbers are similar in that the real line is extended to the twodimensional plane with the additional infinitessimal coordinate, $\epsilon$, along with the rule that $\epsilon^{2}=0$, much as $i^{2}=-1$ in complex numbers.
 Section 15. 

 Probability and probabilistic graphical models

Relevant reference reading material for this section are MLSP, Section 1.4, Section 1.6, Section 5.1, Section 5.2, PRML, Section 8.1, Section 8.2 and R\&N, Section 14.1, Section 14.2 .

 15.1. Rules of probability and probability distributions

The AI and ML algorithms discussed this far in the course are not statistically precise, but, particularly in scientific or engineering applications, we need a quantifiable treatment of uncertainty, and this requires probability. Probability provides a mathematical calculus for reasoning about random events, conveniently indexed using random variables (RV) which can be discrete or continuous. Probability functions obey the rules (axioms) of probability, which are:

1. $\operatorname{Pr}(A) \geq 0$,
2. $\operatorname{Pr}(A \cup B)=\operatorname{Pr}(A)+\operatorname{Pr}(B)$ provided $A \cap B=\emptyset$,
3. $\operatorname{Pr}(\Omega)=1$.

Here, $\operatorname{Pr}(A)$ refers to the probability of random event $A, \Omega$ refers to the set of all possible events ${ }^{1}$ which is also known as the sample space of the model, and $\emptyset$ is the empty set (the impossible event). In words, the first rule states that probabilities are non-negative real numbers. The second states that the probabillity of non-overlapping events is the sum of their individual probabilities. The third states that the probability of any event occurring, is 1, i.e. it is inevitable that some event occurs.

Probability models come in two main kinds: discrete and continuous. Discrete probability mass functions (PMF) give the probability that the RV $X$ takes on the value $x$, written in shorthand as $P(X=x)$. It is an actual probability so it lies in the range $[0,1]$. Furthermore it must be normalized, so $\sum_{x \in \Omega_{X}} P(X=x)=1$, where $\Omega_{X}$ is the sample space for $X$. A continuous probability density function (PDF) $p(x)$ gives the amount of probability per unit (probability density). A PDF must satisfy $p(x) \geq 0$ for all $x \in \Omega_{X}$. The volume under this function (a generalization of the area under a unidimensional PDF ), gives the probability of the event represented by that volume and is calculated using integration, $\int_{x \in A} p(x) d x=\operatorname{Pr}(A)$ and it must be normalized, $\int_{\Omega_{X}} p(x) d x=1 .{ }^{2}$ For the purposes of this course "summation for discrete, integration for continuous" is a good heuristic under which nearly all of probability calculus is the same for both discrete and continuous distributions. Some examples of discrete RVs follow.

[^0]The Bernoulli distribution, which is a binary RV, is a good model for a 'biased coin flip'. With the sample space $\Omega_{X}=\{0,1\}$, the associated probability (mass) function is given by $P(X=0)=1-p$ and $P(X=1)=p$, where $p \in[0,1]$ is the only parameter. This is normalized because,

$$
\begin{align*}
\sum_{x \in\{0,1\}} P(X=x) & =P(X=0)+P(X=1) \\
& =1-p+p  \tag{15.1}\\
& =1
\end{align*}
$$

The case $p=\frac{1}{2}$ represents a fair coin.

The (discrete) uniform distribution has a finit

e sample space such as $\Omega_{X}=\{1,2, \ldots, N\}$, where each event is equally likely, $P(X=x)=\frac{1}{N}$, so it is normalized, $\sum_{x=1}^{N} P(X=x)=\sum_{x=1}^{N} \frac{1}{N}=1$. An obvious example is the fair dice with $N=6$, so $P(X=x)=\frac{1}{6}$.

The uniform distribution is a special case of the categorical distribution where each outcome can have a different probability, so $P(X=x)=p_{x}$, which means there are $N$ parameter values in the length $N$ vector, indexed by $x$. Each $p_{x}$ is a probability value, so it must satisfy $0 \leq p_{x} \leq 1$. For example, for $N=3$, the categorical distribution $p=[0.25,0.25,0.5]$ has $p_{1}=0.25, p_{2}=0.25$ and $p_{3}=0.5$.

A more complex distribution is the binomial distribution which can be considered a model of a sequence of independent, but biased, coin flips (i.e. Bernoulli distributed events). It can be shown that the probability of any sequence of flips with $x \leq N$ heads is $(1-p)^{N-x} p^{x}$. The number of ways of obtaining $x$ heads in this sequence is given by the binomial coefficient,

$$
\begin{equation*}
\binom{N}{x}=\frac{N!}{x!(N-x)!} \tag{15.2}
\end{equation*}
$$

This is the number of combinations of $k$ elements among $N$, from which we obtain the binomial mass function,

$$
\begin{equation*}
P(X=x)=\binom{N}{x}(1-p)^{N-x} p^{x} \text {. } \tag{15.3}
\end{equation*}
$$

Perhaps the most ubiquitous continuous distribution, is the Gaussian or normal distribution. In the $D=1$ case, this has the density function, ${ }^{3}$

$$
p(x)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right)
$$

on the sample space $\Omega_{X}=\mathbb{R}$, the whole of the real line. The parameter $\mu \in \mathbb{R}$ is the mean or average of the distribution, which locates the highest probablity density value of the function, and $\sigma^{2}>0$ is the variance which measures the 'width' or 'spread' of the density around the mean (Figure 15.1).

The Gaussian has many remarkable properties. Among these: the mean, median (value which has as much probability on one side as the other) and mode (the highest probability value) are all equal to $\mu$, and the distribution of a sum of an infinite number of random variables (with finite mean and variance), is Gaussian (via the famous central limit theorem). For this reason (and many others), the Gaussian is widely used in probabilstic ML and AI.

\footnotetext{
${ }^{3}$ This normal distribution is a special case of the multivariate Gaussian for $D>1$.

![](https://cdn.mathpix.com/cropped/2024_07_30_a85a8bdeccdc3c134dcag-3.jpg?height=709&width=1027&top_left_y=328&top_left_x=503)

Figure 15.1.: Density function (PDF) of the Gaussian distribution for a variety of mean $\mu$ and variance $\sigma^{2}$ parameter values.

 15.2. Two or more random variables together: marginals, conditionals, independence

Two or more events occurring simultaneously are represented by joint RVs and corresponding joint PMFs and/or PDFs. For the case of two discrete RVs, $X$ and $Y$, the joint mass function is,

$$
\begin{equation*}
\operatorname{Pr}(X=x \text { and } Y=y)=P(X=x, Y=y) \tag{15.4}
\end{equation*}
$$

defined on the joint sample space $\Omega_{X} \times \Omega_{Y}$. As an example, consider the case of a coin flip paired with a dice throw, the joint sample space has $2 \times 6=12$ possible outcomes,

$$
\Omega_{X} \times \Omega_{Y}=\{(H, 1),(H, 2),(H, 3),(H, 4),(H, 5),(H, 6),(T, 1),(T, 2),(T, 3),(T, 4),(T, 5),(T, 6)\}
$$

As a proper distribution function, the joint distribution must be normalized, so

$\sum_{(x, y) \in \Omega_{X} \times \Omega_{Y}} P(X=x, Y=y)=1$. For two continuous RVs, the joint PDF $p(x, y)$ is the density value at $x \in \Omega_{X}$ and $y \in \Omega_{Y}$ simultaneously. Clearly, joint density functions should be normalized, so we must have $\int_{\Omega_{X} \times \Omega_{Y}} p(x, y) d x d y=1$. Since the laws of probability apply equally to the discrete and continuous cases, we will usually use the shorthand $P_{X Y}(X, Y)$ for the joint distribution function of the random variables $X$ and $Y$, or where there is no ambiguity just $P(X, Y)$.

A joint distribution contains all information about the joint RVs, so for instance, we can find the distribution function of one of the RVs from the joint, by marginalization,

$$
\begin{align*}
& P(Y=y)=\sum_{x \in \Omega_{X}} P(X=x, Y=y) \\
& P(X=x)=\sum_{y \in \Omega_{Y}} P(X=x, Y=y) \tag{15.5}
\end{align*}
$$

and the same result applies to continuous distributions using integration rather than summation.

| Joint $P(X=x, Y=y)$ | $y=0$ | $y=1$ |
| :--- | :--- | :--- |
| $x=0$ | $\frac{3}{7}$ | $\frac{1}{7}$ |
| $x=1$ | $\frac{3}{15}$ | $\frac{8}{35}$ |


| Marginal $P(X=x)$ |  |
| :--- | :--- |
| $x=0$ | $P(X=0, Y=0)+P(X=0, Y=1)=\frac{4}{7}$ |
| $x=1$ | $P(X=1, Y=0)+P(X=1, Y=1)=\frac{3}{7}$ |
| Marginal $P(Y=y)$ |  |
| $y=0$ | $P(X=0, Y=0)+P(X=1, Y=0)=\frac{22}{35}$ |
| $y=1$ | $P(X=0, Y=1)+P(X=1, Y=1)=\frac{13}{35}$ |


| Conditional $P(X=x \mid Y=y)$ | $y=0$ | $y=1$ |
| :--- | :--- | :--- |
| $x=0$ | $\frac{P(X=0, Y=0)}{P(Y=0)}=\frac{15}{22}$ | $\frac{P(X=0, Y=1)}{P(Y=1)}=\frac{5}{13}$ |
| $x=1$ | $\frac{P(X=1, Y=0)}{P(Y=0)}=\frac{7}{22}$ | $\frac{P(X=1, Y=1)}{P(Y=1)}=\frac{8}{13}$ |
| Conditional $P(Y=y \mid X=x)$ | $y=0$ | $y=1$ |
| $x=0$ | $\frac{P(X=0, Y=0)}{P(X=0)}=\frac{3}{4}$ | $\frac{P(X=0, Y=1)}{P(X=0)}=\frac{1}{4}$ |
| $x=1$ | $\frac{P(X=1, Y=0)}{P(X=1)}=\frac{7}{15}$ | $\frac{P(X=1, Y=1)}{P(X=1)}=\frac{8}{15}$ |

Table 15.1.: Illustrating all marginal and conditional distributions for a joint categorical distribution over the joint sample space $\Omega_{X} \times \Omega_{Y}=\{0,1\} \times\{0,1\}$.

| Joint $P(X=x, Y=y)$ | $y=0$ | $y=1$ | Marginal $P(X=x)$ |  | Marginal $P(Y=y)$ |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $x=0$ | 0.30 | 0.02 | $x=0$ | 0.32 | $y=0$ | 0.38 |
| $x=1$ | 0.08 | 0.60 | $x=1$ | 0.68 | $y=1$ | 0.62 |
| Factored joint $P(X=x) P(Y=y)$ |  |  | $y=0$ |  | $y=1$ |  |
| $x=0$ |  |  | $P(X=0) P(Y=0)=0.122$ |  | $P(X=0) P(Y=1)=0.198$ |  |
| $x=1$ |  |  | $P(X=1) P(Y=0)=0.258$ |  | $P(X=1) P(Y=1)=0.422$ |  |

Table 15.2.: Illustrating independence between random variables, over the joint categorical sample space $\Omega_{X} \times \Omega_{Y}=\{0,1\} \times\{0,1\}$. The fact that $P(X, Y) \neq P(X) P(Y)$ means that $X$ and $Y$ are not independent.

Properly normalized, fixing one variable allows the joint to act as a new distribution called the conditional,

$$
\begin{align*}
& P(X=x \mid Y=y)=\frac{P(X=x, Y=y)}{P(Y=y)} \\
& P(Y=y \mid X=x)=\frac{P(X=x, Y=y)}{P(X=x)} \tag{15.6}
\end{align*}
$$

and we use the shorthand $P(X \mid Y)$ and $P(Y \mid X)$. Both marginals and conditionals can be extended to multiple joint random variables through summation or division by joint subsets. See Table 15.1 for a worked example in the case of the joint categorical distribution.

If the probability of a conditional distribution $P(X \mid Y)$ is unaffected by the conditioning variable $Y$, we say that $X$ is independent of $Y$. In other words, if $P(X \mid Y)=P(X)$, then we can rearrange this to write $P(X, Y)=P(X) P(Y)$ so the joint distribution is factored into a product of the marginal distributions (Table 15.2).

![](https://cdn.mathpix.com/cropped/2024_07_30_a85a8bdeccdc3c134dcag-5.jpg?height=450&width=449&top_left_y=289&top_left_x=809)

Figure 15.2.: Simple example probabilistic graphical model over the five RVs $V, U, X, Y$ and $Z$. We can immediately see the child/parent relationships between variables; some of them: $V$ is a parent of both $U$ and $Z, Y$ is a child of $X$ and $X$ is a child of $U$ in turn. We can therefore read off the conditional independence relationships, for example, $P(Z \mid V, U)=P(Z \mid V, U, X, Y)$ (in words, $Z$ is conditionally independent of $X$ and $Y$ given $U$ and $V), P(Y \mid X)=P(Y \mid X, Z)$ ( $Y$ is conditionally independent of $Z$ given $X$ ).

Independence/conditional independence such as this, is of critical important in probabilistic AI and ML, since it allows joint RVs to be efficiently modelled by subsets of the data. This is the basis of probabilistic graphical models (PGMs), which we explore in detail in the next section. We will study several examples of PGMs for practical ML algorithms in later sections.

 15.3. Probabilistic graphical models (PGMs)

Given a probability distribution with multiple RVs, we can represent their mutual conditional independence graphically using a probabilistic graphical model (PGM). This is an directed acyclic graph (DAG) in which the nodes in the graph are the RVs such as $X, Y, Z$ etc. and the edges $X \rightarrow Y$ represent the conditional dependence relationship, $P(Y \mid X)$, between them. We say that $X$ is a parent of $Y$ in the graph, and $Y$ is a child of $X$. Stated another (equivalent) way, which is often more useful for modelling purposes in ML, if $V$ has some parent variables in a PGM, but there is no edge $U \rightarrow V$, then $V$ is conditionally independent of $U$, given its parents. PGMs have widespread use in probabilistic AI and ML, there are special graph structures for applications such as time series or image data. See Figure 15.2 for a simple example PGM.

Every joint distribution over a set of multiple RVs, can be expressed using the chain rule of probability ${ }^{4}$, which is a product of their conditionals. For example, in the three variable case there are six different 'chain-factored' expressions,

$$
\begin{align*}
P(X, Y, Z) & =P(X \mid Y, Z) P(Y \mid Z) P(Z) \\
& =P(Y \mid X, Z) P(X \mid Z) P(Z) \\
& =P(Y \mid Z, X) P(Z \mid X) P(X) \\
& =P(Z \mid Y, X) P(Y \mid X) P(X)  \tag{15.7}\\
& =P(Z \mid X, Y) P(X \mid Y) P(Y) \\
& =P(X \mid Z, Y) P(Z \mid Y) P(Y)
\end{align*}
$$

\footnotetext{
${ }^{4}$ Not to be confused with the chain rule of distribution!

In fact for $N$ variables there are $N$ ! such factorizations, one for every permutation of the variables. Note that these factorizations of the joint expression into products of conditionals, are true regardless of the conditional independence properties of their joint distribution. ${ }^{5}$ What makes this chain factorization useful is when it is combined with the parent/child structure of the graph and the conditional independence properties it dictates. We can find a sequence of variables e.g. $[X, U, V]$ or $[U, V, X]$, such that, every variable in the sequence occurs after its parents (or before its children), in the DAG. Such a sequence (there can be more than one of them) is called a topological ordering of the DAG. For instance, in Figure 15.2, a topological ordering is $[V, U, X, Y, Z]$. This ordering is a permutation, so we can now write down the corresponding chain factorization according to that permutation. For instance, according to the permutation $[V, U, X, Y, Z]$, we have the specific chain factorization,

$$
\begin{equation*}
P(V, U, X, Y, Z)=P(V) P(U \mid V) P(X \mid U, V) P(Y \mid X, U, V) P(Z \mid Y, X, U, V) \tag{15.8}
\end{equation*}
$$

However, note that, according to the conditional independence properties implied by the PGM, we can simplify some of the above conditionals,

$$
\begin{align*}
P(Z \mid Y, X, U, V) & =P(Z \mid U, V) \\
P(Y \mid X, U, V) & =P(Y \mid X)  \tag{15.9}\\
P(X \mid U, V) & =P(X \mid U)
\end{align*}
$$

which can now be inserted back into (15.8), to obtain the simpler factorization,

$$
\begin{equation*}
P(V, U, X, Y, Z)=P(V) P(U \mid V) P(X \mid U) P(Y \mid X) P(Z \mid U, V) \tag{15.10}
\end{equation*}
$$

This factorization (which is unique) ${ }^{6}$, is known as the Markov factorization of the PGM. Given the PGM, we can simply 'read off' this factorization directly.

 Learning outcomes for section 15

$\triangleright$ Demonstrate knowledge of the fundamental principles of probability and probability distributions: continuous and discrete sample spaces, random variables, distribution functions, normalization, joint random variables, marginalization, conditioning, chain rule.

$\triangleright$ Explain the form and meaning of the distribution function for Bernoulli, categorical, binomial and Gaussian distributions.

$\triangleright$ Demonstrate understanding of conditional independence and probabilistic graphical models: be able to factorize a joint distribution according to a graphical model.

[^1]
[^0]:    ${ }^{1}$ As a technical point of detail, events can be combinations of outcomes, including single outcomes, which are elements of the sigma-algebra on the sample space.

    ${ }^{2}$ Note that this does not mean that $p(x) \leq 1$ !

[^1]:    ${ }^{5}$ They simply follow from the definition of conditionals, e.g. $\quad P(X, Y)=P(X \mid Y) P(Y)$ because $P(X \mid Y)=$ $P(X, Y) / P(Y)$ so $P(X, Y)=P(X, Y) \times P(Y) / P(Y)=P(X, Y)$ as required.

    ${ }^{6}$ But note that the ordering of the terms in the expression is not unique, due to the commutativity of multiplication.

 Section 16. 

 Bayesian models

Relevant reference reading material for this section is MLSP, Section 1.4, PRML, Section 1.2 and $\mathbf{R} \& \mathbf{N}$, Section 20.2.2.

 16.1. Bayes' theorem

Given a conditional distribution and the corresponding marginal for that variable, we can swap conditionals,

$$
\begin{equation*}
P(X \mid Y)=\frac{P(Y \mid X) P(X)}{P(Y)} \tag{16.1}
\end{equation*}
$$

and similar for $P(Y \mid X)$. This is known as Bayes' theorem and is so widely used that the terms in this equation have special names. $P(X)$ is called the prior, $P(Y \mid X)$ the likelihood, $P(Y)$ evidence and $P(X \mid Y)$ is the posterior. For practical reasons, if we do not know the evidence distribution, it is often presented in the following, equivalent form, ${ }^{1}$

$$
\begin{equation*}
P(X \mid Y)=\frac{P(Y \mid X) P(X)}{\sum_{x \in \Omega_{X}} P(Y \mid X=x) P(X=x)} \tag{16.2}
\end{equation*}
$$

Bayes' theorem is just a mathematical fact, but it has critical applications for rational decisionmaking under uncertainty in probabilistic AI and ML. Here is an example which illustrates the point. Consider the problem of determining probabilities of a range of health states of an individual, given a symptom. Precision computations are required because one of the outcomes is meningitis which is a life-threatening condition: it should be quickly treated and will often require hospitalization, but again a false positive has substantial cost.

To set up a probabilistic model, assume an RV $S=0$ if an individual does not have a specific symptom and $S=1$ if they have that symptom ( $S$ is Bernoulli-distributed with sample space $\Omega_{S}=$ $\{0,1\})$. This symptom could prompt three possible outcomes $-D=h$, healthy, $D=c$ influenza and $D=m$ for meningitis ( $D$ has a categorical distribution with sample space $\Omega_{D}=\{h, c, m\}$ ). Assume the likelihoods are known, i.e. the probability of a specific health state given the symptom is present:

$$
\begin{array}{r}
P(S=1 \mid D=h)=0.1 \\
P(S=1 \mid D=c)=0.5  \tag{16.3}\\
P(S=1 \mid D=m)=0.9
\end{array}
$$

From the general medical literature, the priors for the health state are known (that is, the background probability of each health state in the population at large):

\footnotetext{
${ }^{1}$ To see why this is correct, note that $P(Y \mid X) P(X)=P(Y, X)$ so it is just the marginal.

$$
\begin{align*}
P(D=h) & =0.9 \\
P(D=c) & =0.09  \tag{16.4}\\
P(D=m) & =0.01
\end{align*}
$$

This reflects the fact that at any one time, most people do not have influenza, and meningitis is particularly rare. Obviously, the symptom is dependent upon the health state. This information is enough to construct a two-node PGM with:

$$
\begin{equation*}
P(S, D)=P(S \mid D) P(D) \tag{16.5}
\end{equation*}
$$

Now, using Bayes' theorem (16.1), we can compute the posterior probability of each health state, given that an individual has the symptom:

$$
\begin{equation*}
P(D \mid S=1)=\frac{P(S=1 \mid D) P(D)}{P(S=1)} \tag{16.6}
\end{equation*}
$$

The evidence (the marginal symptom probability) $P(S=1$ ) is not known, but (16.2) can be used instead,

$$
\begin{align*}
P(S=1) & =P(S=1 \mid D=h) P(D=h)+P(S=1 \mid D=c) P(D=c) \\
& +P(S=1 \mid D=m) P(D=m) \tag{16.7}
\end{align*}
$$

After some arithmetic, the result is $P(S=1)=0.14$ from which we get the posterior probabilities $P(D=h \mid S=1)=0.63, P(D=c \mid S=1)=0.31$ and $P(D=m \mid S=1)=0.06$.

Initially, going on the prior information (16.4) alone would not suggest any cause for alarm. However, on observing the likelihoods, ranked in order of most probable outcome, while being healthy is still the most probable decision which the likelihoods do not change, both influenza and meningitis posterior probabilities are substantially higher. For instance, although the prior probability of meningitis is only $1 \%$, this has been multiplied six-fold after considering the high likelihood of observing the symptom if the individual actually has meningitis. Bayes' theorem is thus a rational process which precisely synthesizes disparate sources of uncertain information.

 16.2. Probabilistic classification using Bayes' theorem

Probabilistic classification can be expressed as an application of Bayes' rule: given some input (feature) data $X=x$, determine the probability $P(Y \mid X)$ of the class $Y=y$ for $y \in \Omega_{Y}$ (where $\Omega_{Y}$ is a discrete sample space) to which $X$ belongs (posterior), taking into account $P(Y)$ (prior) how probable that class is before having seen the data $P(X \mid Y)$. A rational decision is to select the value of $Y$ which maximizes the posterior $P(Y \mid X)$, which is called the maximum a-posteriori (MAP) decision rule:

$$
\begin{equation*}
y^{\star}=\underset{y \in \Omega_{Y}}{\arg \max } P(Y=y \mid X=x) \tag{16.8}
\end{equation*}
$$

Applying Bayes' theorem to this equation we can express this as,

$$
\begin{equation*}
y^{\star}=\underset{y \in \Omega_{Y}}{\arg \max } P(X=x \mid Y=y) P(Y=y) \tag{16.9}
\end{equation*}
$$

![](https://cdn.mathpix.com/cropped/2024_07_30_bd3610f74bd1741e4f7bg-3.jpg?height=364&width=629&top_left_y=309&top_left_x=719)

Figure 16.1.: Explaining Bayesian probabilistic inference in machine learning, for example in the context of classification. Whereas (as the PGM indicates) the computation of feature data depends upon knowledge of the class, Bayesian inference such as MAP decision-making, computes information about the class knowing the feature data, thus reversing the edge direction (orange arrow).

![](https://cdn.mathpix.com/cropped/2024_07_30_bd3610f74bd1741e4f7bg-3.jpg?height=309&width=643&top_left_y=939&top_left_x=638)

Feature

data

Figure 16.2.: The probabilistic graphical model (PGM) of the naive Bayes classifier, which simplifies Bayesian classification by assuming each feature dimension, $X^{1}, X^{2}, \ldots, X^{D}$ is independent of the others, given the class.

where the denominator in (16.2) can be omitted because the evidence probability $P(X=x)$ does not depend upon $y .{ }^{2}$ We can view this decision as a form of probabilistic inference (as contrasted with logical inference in symbolic AI), where Bayes' rule is used to reverse the direction of the flow of computation (feature from class, rather than class from feature, Figure 16.1).

Using (16.9) in practical applications requires knowing the likelihood distribution $P(X \mid Y)$. In general, the input feature data $X$ can be multidimensional, i.e. a $D$-dimensional vector, and there is no guarantee that these dimensions will be independent of one another. This makes it challenging to estimate the likelihood from data. As a convenient simplification, the naive Bayes' classifier assumes that each feature is conditionally independent of the others, given the class. The corresponding PGM is shown in Figure 16.2.

Reading off the Markov factorization from the naive Bayes' PGM, we have,

$$
\begin{equation*}
P(X \mid Y)=P\left(X^{1} \mid Y\right) \times P\left(X^{2} \mid Y\right) \times \cdots \times P\left(X^{D} \mid Y\right) \tag{16.10}
\end{equation*}
$$

[^0]Now, insert this quantity into Bayes' rule, to obtain, ${ }^{3}$

$$
\begin{align*}
P(Y \mid X) & =\frac{P\left(X^{1} \mid Y\right) \times P\left(X^{2} \mid Y\right) \times \cdots \times P\left(X^{D} \mid Y\right) \times P(Y)}{P(X)}  \tag{16.11}\\
& \propto P\left(X^{1} \mid Y\right) \times P\left(X^{2} \mid Y\right) \times \cdots \times P\left(X^{D} \mid Y\right) \times P(Y)
\end{align*}
$$

and as previously, since $P(X)$ is independent of $y$,

$$
\begin{equation*}
y^{\star}=\underset{y \in \Omega_{Y}}{\arg \max }\left(P\left(X^{1}=x^{1} \mid Y=y\right) \times P\left(X^{2}=x^{2} \mid Y=y\right) \times \cdots \times P\left(X^{D}=x^{D} \mid Y=y\right) \times P(Y=y)\right) . \tag{16.12}
\end{equation*}
$$

It is worth interpreting the meaning of this equation. Given some new test data as a set of values from the vector $x=\left[x^{1}, x^{2}, \ldots, x^{D}\right]$, for each possible class $y \in \Omega_{Y}$, the product of the conditional likelihood $P(X \mid Y)$ is evaluated, and they are all multiplied together along with the class prior probability, $P(Y)$. Then the class which has the highest likelihood probability when evaluated on the test data $x$, weighted by the prior for that class, is selected as the classification decision associated with the given test data. So, if two classes have similar likelihood on the test data, the decision about which one to choose will be dominated by the relative probability of their priors. On the other hand, if the class priors are very similar, it is their relative likelihoods which most influence the class selection. This is another example of the rational fusion of different sources of uncertain information which is the hallmark of Bayesian inference in ML.

Naive Bayes' is a surprisingly good classifier for high-dimensional problems (where $D$ is large), since it does not require a large amount of training data. Estimating feature distribution parameters is very quick, in fact, $O(D)$. Making a prediction requires evaluating $D$ times $\left|\Omega_{Y}\right|$ (the number of classes), which is usually easy to carry out in practice. Nonetheless, the assumption of conditional feature independence is unrealistic for many practical ML problems.

 16.3. Example: identifying pulsars

Pulsars are strange, extremely dense, astronomical objects that can be formed from neutron stars that spin rapidly, emitting beams of electromagnetic radiation as they do. They are called pulsars because the signal they emit sweeps across a telescopes' field of view creating a 'pulsating' (oscillating) signal. Detecting pulsars requires finding their signature in astronomical observations, but most detected signals which could be pulsars are just some kind of spurious radio interference masquerading as genuine pulsars. The problem of finding genuine pulsars from among highly diverse radio noise is not easily posed as a simple software problem because the interference can be arbitrarily complex and pulsars have multiple different sorts of oscillation 'signatures'. Furthermore, in modern digital sky surveys it is possible to identify many thousands of candidate signals which could be pulsars; sorting out spurious noise from actual pulsars is a laborious process. This is an ideal task for supervised, probabilistic ML classification because the genuine and spurious pulsar signatures have a fairly significant level of uncertainty but it is relatively easy to check pulsars by hand which can then be used as training data for a Bayesian classifier.

To apply the naive Bayes classifier to the HTRU2 pulsar dataset ${ }^{4}$ we first need to decide on the conditional models for each putative pulsar feature. For simplicity we will use Gaussian distribution models because most of the features commonly used to identify pulsars are continuous. In the case of

[^1]![](https://cdn.mathpix.com/cropped/2024_07_30_bd3610f74bd1741e4f7bg-5.jpg?height=895&width=1157&top_left_y=295&top_left_x=444)

Figure 16.3.: Using naive Bayes' classification to solve the problem of automatically detecting pulsars among spurious radio interference from the automated High Time Resolution Universe digital sky survey. The conditional class likelihood models for each of the two features are Gaussian, where the means of each Gaussian under the two detection condition classes ( $y=1$, spurious interference and $y=2$, genuine pulsar) are shown in the distribution panels. The centre panel shows a point in $X^{1}-X^{2}$ space for each potential pulsar candidate in the dataset.

two features, $X^{1}$ (excess kurtosis of the integrated candidate pulsar profile) and $X^{2}$ (excess kurtosis of the DM-SNR curve), we need the mean and variance parameters for each, conditional on each class, $Y=1$ (spurious) and $Y=2$ (pulsar). Splitting the training data into two halves, the estimate of on the training set gives, for feature $X^{1}$ in the spurious and pulsar cases, $\mu_{x 1 y 1}=0.2$ and $\mu_{x 2 y 1}=8.70$, respectively. For feature $X^{2}$, the same means are $\mu_{x 1 y 2}=3.1$ and $\mu_{x 2 y 2}=2.86$. Similarly, for variances the parameters are $\sigma_{x 1 y 1}^{2}=0.11, \sigma_{x 2 y 1}^{2}=17.15, \sigma_{x 1 y 2}^{2}=3.49$ and $\sigma_{x 2 y 2}^{2}=8.50$. The class priors are just $p_{y 1}=0.87$ and $p_{y 2}=0.13$. We can see immediately very substantial class imbalance in this case (because most of the potential pulsar signals are just radio interference). It is important to visualize the training dataset and conditionally independent feature models together (Figure 16.3).

Having these feature model parameters immediately allows computation of the test set performance of the naive Bayes' classifier, by applying the inference formula (16.12) to obtain the most probable class $y_{i}^{\star}$ for each data set item $x=\left[x^{1}, x^{2}\right]$ in the test split of the dataset. The $0-1$ classification error can then be calculated as $F_{01}=\sum_{i=1}^{N} \mathbb{I}\left[y_{i}^{\star} \neq y_{i}\right]$ where $y_{i}$ is the known label. In this case, the naive Bayes' classifier has $F_{01}=1181$ classification errors in test set size $N=8950$ candidates, giving a test set accuracy of approximately $80 \%$.

This is quite good accuracy for such a simple model, but not sufficiently accurate for scientific purposes in astronomy. It would be reasonable to trust those pulsar candidates with high posterior probability, which can be calculated by dividing through by the evidence probability $P\left(x^{1}, x^{2}\right)=$ $\sum_{y \in\{0,1\}} P\left(X^{1}=x^{1} \mid Y=y\right) P(Y=y)$ in each test case. In fact for this model, $91 \%$ of the predicted cases have posterior probability of $75 \%$ or higher, thus leaving only about $10 \%$ of cases which might
need to be checked by hand (i.e. about 900 cases out of nearly 9000), very substantially reducing the labour required to obtain good quality scientific evidence.

 Learning outcomes for section 16

$\triangleright$ Show understanding of Bayes' theorem: prior, posterior, evidence and likelihood distributions, swapping conditionals.

$\triangleright$ Demonstrate the principle of MAP inference as applied to probabilistic classification.

$\triangleright$ Explain the rational behind, probabilistic structure of, and method of, naive Bayes classification: apply naive Bayes' to small classification problems.


[^0]:    ${ }^{2}$ This is actually a general fact about $\arg \max$ and $\arg \min$ : they are unchanged if the quantity being maximized is multiplied (or divided) by a value which is independent of the variable over which the quantity is being maximized.

[^1]:    ${ }^{3}$ This can also be expressed using iterated products as $P(Y \mid X)=\prod_{d=1}^{D} P\left(X^{d} \mid Y\right) P(Y)$.

    ${ }^{4}$ https://archive.ics.uci.edu/dataset/372/htru2


 Section 17 . 

 Sequential problems and hidden Markov models

Relevant to this section is the material in PRML, Section 13.2, MLSP, Section 9.4 and R\&N, Section 15.3 (matrix algebra approach to HMMs).

 17.1. Markov chains and the hidden Markov model

Many applications of ML involve ordered data that is, data for which the ordering matters. Examples include:

$\triangleright$ Natural language (ordered sequences of words)

$\triangleright$ Appointment calendar entries (date and time-ordered event names)

$\triangleright$ Medical symptom diaries (e.g. time-ordered records of pain measurements)

$\triangleright$ Electronic health records (time-ordered sequences of medical system interactions)

$\triangleright$ Macroeconomic time series (time-ordered sequences of GDP values)

$\triangleright$ Genomics (base pairs in a genome sequence)

What has been shown is that ML algorithms such as regression and classification are more useful if they take into account this ordering. For probabilistic $\mathrm{AI} / \mathrm{ML}$ this requires special kinds of PGMs, a primary example of which is the so-called Markov chain in which a sequence of RVs $Y_{0}, Y_{1}, \ldots, Y_{T}$ is organized such that the next one in the sequence only depends upon the previous one, i.e. the PGM where,

$$
\begin{equation*}
P\left(Y_{t} \mid Y_{t-1}, Y_{t-2}, \ldots, Y_{0}\right)=P\left(Y_{t} \mid Y_{t-1}\right) \tag{17.1}
\end{equation*}
$$

for all $t=1,2, \ldots, T$, along with the independent $Y_{0}$ to get the iteration started. If there are many sequences of data then (17.1) can be fitted to the data and used to make predictions. In practical applications, however, it is more common that the sequence $Y_{0}, Y_{1}, \ldots, Y_{T}$ is not actually measured. The appropriate PGM for this situation is called the hidden Markov model (HMM) (Figure 17.1), where there is an edge $Y_{t-1} \rightarrow Y_{t}$ and another $Y_{t} \rightarrow X_{t}$ are called hidden states and the variables $Y_{t}$ the observations. Typically, the hidden states are discrete with finite sample space $\Omega_{Y}$. The observations therefore only depend upon the hidden state at the same time instant.

In applications where HMM are used, there is the need to solve the following problems:

$\triangleright$ Model fitting: given only the sequence of observed data $X_{0}, X_{1}, \ldots, X_{T}$, estimate the distribution functions in the PGM, i.e. find $P\left(X_{t} \mid Y_{t}\right)$ (the distribution which models how the observed data depends upon the hidden states) and $P\left(Y_{t} \mid Y_{t-1}\right)$ (the distribution of the current state given the previous one),

![](https://cdn.mathpix.com/cropped/2024_07_30_7cba98d7a35fdf29e4a8g-2.jpg?height=401&width=969&top_left_y=308&top_left_x=543)

Figure 17.1.: The probabilistic graphical model (PGM) of the hidden Markov model (HMM).

$\triangleright$ Evaluation: given fixed model parameters and complete sequence of observed data, compute the probability of the complete sequence of observed data, $P(X)$, and,

$\triangleright$ Decoding: given fixed model parameters, and data compute the most probable sequence of hidden states, $y^{\star}=\left[y_{0}^{\star}, y_{1}^{\star}, \ldots, y_{T}^{\star}\right]$.

On the face of it, solving these problems requires evaluating all possible sequences of hidden states which we denote by $\mathcal{Y}$; if there are $K$ hidden states, this means $O\left(K^{T}\right)$ (exponential complexity). However, the use of dynamic programming (Section 5) makes this tractable, leading to efficient, lineartime $O\left(T K^{2}\right)$ inference algorithms.

 17.2. Viterbi decoding

This course will investigate the efficient Viterbi decoding algorithm for HMMs. Stated mathematically, the problem to be solved is,

$$
\begin{equation*}
P\left(y^{\star}\right)=\max _{y^{\prime} \in \mathcal{Y}} P\left(X_{0}=x_{0}, \ldots, X_{T}=x_{T}, Y_{0}=y_{0}^{\prime}, \ldots, Y_{T}=y_{T}^{\prime}\right) \tag{17.2}
\end{equation*}
$$

where $x_{0}, x_{1}, \ldots, x_{T}$ is the set of observed data, $y=\left[y_{0}, y_{1}, \ldots, y_{T}\right]$ is a hidden sequence, and the configuration space $\mathcal{Y}$ is the set of all possible such sequences of hidden states over time $t=0$ to $t=T$. For example, with $\Omega_{Y}=\{1,2,3\}$ so that $K=3$ and $T=3$, then particular hidden sequences are $y=[1,2,1,3]$ and $y=[3,2,2,1]$. The hidden state can change from any of the $K$ states at time $t$, to any of the $K$ states at time $t+1$. So, if there are $t=0,1, \ldots, T$ time stages, there are $K^{T+1}$ possible sequences, an intractable exponential number of possible sequences over which to maximize (17.2).

However, there is a factorized SDP for these hidden sequences, through which we can use dynamic programming to make this maximization tractable. Assume we have the sequence of states for stage $t-1$. The $K$ new sequences of states at stage $t$, is simply obtained from each sequence at stage $t-1$, by appending each of the $K$ possible states. Now, considering the HMM graphical model up to stage $t-1$, associated with the sequence of states is the (joint) optimal distribution (for clarity, we drop the assignment of observed data to their random variables),

$$
\begin{equation*}
P^{\star}\left(X_{0}, \ldots, X_{t-1}, Y_{t-1}\right)=\max _{y^{\prime} \in \mathcal{Y}_{t-2}} P\left(X_{0}, \ldots, X_{t-1}, Y_{0}=y_{0}^{\prime}, \ldots, Y_{t-2}=y_{t-2}^{\prime}, Y_{t-1}\right) \tag{17.3}
\end{equation*}
$$

where $\mathcal{Y}_{t-2}$ is the set of all possible hidden sequences up to time $t-2$.

According to the SDP above, associated to every one of the newly generated hidden sequences at each value of $y \in \Omega_{Y}$, is the probability obtained by appending the variables $X_{t}=x_{t}$ and $Y_{t}=y$, to the PGM at stage $t-1$, so that,

$$
\begin{equation*}
P\left(X_{0}, \ldots, X_{t-1}, X_{t}, Y_{t-1}, Y_{t}=y\right)=P\left(X_{0}, \ldots, X_{t-1}, Y_{t-1}\right) P\left(Y_{t}=y \mid Y_{t-1}\right) P\left(X_{t} \mid Y_{t}=y\right) \tag{17.4}
\end{equation*}
$$

Maximizing over $Y_{t}$ leads to,

$$
\begin{equation*}
P^{\star}\left(X_{0}, \ldots, X_{t}, Y_{t}=y\right)=\max _{y^{\prime} \in \Omega_{Y}} P^{\star}\left(X_{0}, X_{1}, \ldots, X_{t-1}, Y_{t-1}=y^{\prime}\right) P\left(Y_{t}=y \mid Y_{t-1}=y^{\prime}\right) P\left(X_{t} \mid Y_{t}=y\right) \tag{17.5}
\end{equation*}
$$

and writing $p_{t}^{\star}(y)=P^{\star}\left(X_{0}, \ldots, X_{t}, Y_{t}=y\right)$ shows that the above is in the form of a Bellman recursion, ${ }^{1}$

$$
\begin{equation*}
p_{t}^{\star}(y)=\max _{y^{\prime} \in \Omega_{Y}} p_{t-1}^{\star}\left(y^{\prime}\right) P\left(Y_{t}=y \mid Y_{t-1}=y^{\prime}\right) P\left(X_{t} \mid Y_{t}=y\right) \tag{17.6}
\end{equation*}
$$

At the final stage $t=T$,

$$
\begin{align*}
\max _{y \in \Omega_{Y}} p_{T}^{\star}(y) & =\max _{y^{\prime} \in \mathcal{Y}_{T}} P\left(X_{0}=x_{0}, \ldots, X_{T}=x_{T}, Y_{0}=y_{0}^{\prime}, \ldots, Y_{T-1}=y_{T-1}^{\prime}, Y_{T}=y\right)  \tag{17.7}\\
& =P\left(y^{\star}\right)
\end{align*}
$$

is the required solution to the HMM optimization problem (17.2). This shows how the optimization problem can be solved efficiently by maximizing over each successive time step, which is an $O\left(T K^{2}\right)$ process, i.e. linear time computation. This process of generating new state sequences by appending states, and corresponding joining of PGMs together which makes the DP recursion possible, is illustrated in Figure 17.2.

To get the recursion (17.6) started, we only need the initial probability function, $p_{0}^{\star}(y)=P\left(X_{0} \mid Y_{0}=y\right) P\left(Y_{0}=y\right)$. The optimal sequence of states $y^{\star}$ can be reconstructed by (a) retaining the optimal decision at each stage, (b) solving for $y_{T}^{\star}$ by optimizing over $p_{T}^{\star}(y)$, then (c) using the retained decisions to work backwards from $y_{T}^{\star}$. Putting this together leads to Viterbi decoding, Algorithm 17.1.

 17.3. Example: genome sequence region segmentation

Genome sequences are made from four DNA base pair molecules: adenosine (A), cytosine (C), guanine (G) and thymine (T). Sub-sequences of three ACGT base pairs encode for different amino acids, which ultimately go to make up proteins which are the building blocks of every part of the living cell. Different sub-sequences of the DNA sequence have different functions in this process, which involves splicing together coding regions called exons (E), and discarding introns (I) and untranslated 5' donor site (D) sub-sequences.

For molecular biology it is very useful to be able to segment a sequence into these diferent subsequences. As the relationship between DNA sequences and coding/non-coding regions is complex, it is well-modelled as an HMM, where the hidden states $\Omega_{Y}=\{e, d, i\}$ are the $K=3$ possible regions,

\footnotetext{
${ }^{1}$ This function has various names in the literature, including the forward optimal message $\alpha(y)$.

![](https://cdn.mathpix.com/cropped/2024_07_30_7cba98d7a35fdf29e4a8g-4.jpg?height=624&width=1357&top_left_y=493&top_left_x=221)

Figure 17.2.: Illustrating the relationship between the SDP which generates new hidden state sequences from previous ones through appending states, and the corresponding process of appending variables and their Markov dependencies, to the PGM to create new, extended PGMs and associated distribution function.


Algorithm 17.1 The Viterbi decoding algorithm for determining the optimal sequence of hidden states in an HMM.

$\triangleright$ Step 1. Initialization: Compute the initial optimal probability function, $p_{0}^{\star}(y)=$ $P\left(X_{0}=x_{0} \mid Y_{0}=y\right) P\left(Y_{0}=y\right)$,

$\triangleright$ Step 2. Forward recursion: Compute the sequence of optimal probability function while keeping track of the corresponding argument function,

$$
\begin{aligned}
p_{t}^{\star}(y) & =\max _{y^{\prime} \in \Omega_{Y}} p_{t-1}^{\star}\left(y^{\prime}\right) P\left(Y_{t}=y \mid Y_{t-1}=y^{\prime}\right) P\left(X_{t}=x_{t} \mid Y_{t}=y\right) \\
Y_{t}^{\star}(y) & =\underset{y^{\prime} \in \Omega_{Y}}{\arg \max } p_{t-1}^{\star}\left(y^{\prime}\right) P\left(Y_{t}=y \mid Y_{t-1}=y^{\prime}\right),
\end{aligned}
$$

for $t=1,2, \ldots, T$,

$\triangleright$ Step 3. Backtracking: Reconstruct the best sequence of hidden states in reverse,

$$
y_{T}^{\star}=\underset{y \in \Omega_{Y}}{\arg \max } p_{T}^{\star}(y), y_{t-1}^{\star}=Y_{t}^{\star}\left(y_{t}^{\star}\right),
$$

for $t=T-1, T-2, \ldots, 0$.

| Stage | $x_{t}$ | $p_{t}^{\star}(y=e)$ | $p_{t}^{\star}(y=d)$ | $p_{t}^{\star}(y=i)$ | $Y_{t}^{\star}(y=e)$ | $Y_{t}^{\star}(y=d)$ | $Y_{t}^{\star}(y=i)$ | $y_{t}^{\star}$ |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| $t=0$ | $g$ | 0.2500 | 0.0000 | 0.0000 |  |  |  | $e$ |
| $t=1$ | $g$ | 0.0563 | 0.0238 | 0.0000 | $e$ | $e$ | $e$ | $e$ |
| $t=2$ | $g$ | 0.0127 | 0.0053 | 0.0024 | $e$ | $e$ | $d$ | $e$ |
| $t=3$ | $g$ | 0.0028 | 0.0012 | 0.0005 | $e$ | $e$ | $d$ | $d$ |
| $t=4$ | $t$ | 0.0006 | 0.0000 | 0.0005 | $e$ | $e$ | $d$ | $i$ |
| $t=5$ | $a$ | 0.0001 | 0.0000 | 0.0002 | $e$ | $e$ | $i$ | $i$ |

Table 17.1.: Example run of Viterbi decoding, Algorithm 17.1, applied to the problem of segmenting DNA sequences of ACGT base pairs, into introns $(i)$, exons $(e)$ and $5^{\prime}$ donor site $(d)$ sub-sequences. The HMM hidden state transition $P\left(Y=y \mid Y=y^{\prime}\right)$, observation distributions $P(X=x \mid Y=y)$ and initial distribution $P\left(Y_{0}=y\right)$ are given in the text.

and the DNA sequence is the observed data $\Omega_{X}=\{a, c, g, t\}$. Knowledge of the molecular biology and experimental work has determined the following state transition and observation distributions,

$$
\begin{align*}
& P\left(Y=y \mid Y=y^{\prime}\right)=\left[\begin{array}{cccc} 
& y=e & y=d & y=i \\
y^{\prime}=e & 0.9 & 0.1 & 0 \\
y^{\prime}=d & 0 & 0 & 1.0 \\
y^{\prime}=i & 0.1 & 0 & 0.9
\end{array}\right] \\
& P(X=x \mid Y=y)=\left[\begin{array}{ccccc}
y=e & x=a & x=c & x=g & x=t \\
y=e & 0.25 & 0.25 & 0.25 & 0.25 \\
y=d & 0.05 & 0 & 0.95 & 0 \\
y=i & 0.4 & 0.1 & 0.1 & 0.4
\end{array}\right] \tag{17.8}
\end{align*}
$$

and the assumption is that the sequence always starts at an exon, e.g. $P\left(Y_{0}=e\right)=1, P\left(Y_{0}=d\right)=0$ and $P\left(Y_{0}=i\right)=0$. This encodes the known biological facts that exons can be followed by donor sites, but not directly by introns; and donor sites can only be followed (immediately) by introns etc. Donor sites consist only of AG base pairs. This is enough information to run the Viterbi algorithm on a given DNA sequence to determine the optimal sequence of segments (Table 17.1). Since this DP algorithm runs in linear time, it can be easily applied to the usually extremely long DNA sequences encountered in biology (i.e. the human genome has about 6 billion base pairs), and is a good example of the ubiquitous use of ML in modern bioinformatics.

An important computational point needs to be raised. The optimal probabilities $p_{t}^{\star}(y)$ eventually become extremely small and difficult to work with computationally, this is a consequence of long sequences leading to long iterated products of probabilities in the recursion (17.6). A practical solution is to work with log-probabilities instead, that is, first take the logarithm of every probability, then since $\ln (p \times q)=\ln p+\ln q$, the recursion involves sums of log-probabilities rather than products, and the Viterbi decoding is otherwise unchanged. ${ }^{2}$

[^0]
 Learning outcomes for section 17

$\triangleright$ Demonstrate knowledge of the structure of the HMM: Markov chains and hidden states, relationship to observed data.

$\triangleright$ Explain the function of the steps of the Viterbi decoding algorithm.

$\triangleright$ Apply the Viterbi decoding algorithm to small sequence decoding problems.


[^0]:    ${ }^{2}$ This works because the maximum distributes over sum as it does over the product, the condition required for the principle of optimality to hold.
 Section 18. 

 Other sequential models

Relevant reference reading material for this section can be found in MLSP, Section 7.5, PRML, Section 13.3 and R\&N, Section 15, Section 17.

 18.1. Optimal sequential decision-making

Making a sequence of choices based on observed effects of those choices, is a common problem in real-world planning and decision-making. Ideally, we want to decide on a sequence of actions which are optimal (the best possible decisions which can be made) with respect to the available information about the effects of these decisions. The problem of computing the optimal decision sequence or policy, can make use of dynamic programming applied to a special kind of sequential graphical model, known as a Markov decision process (MDP) (Figure 18.1).

An MDP represents a sequence of input actions $A_{t}$ which lead to a sequence of observed states, $S_{t}$ for $t=1,2, \ldots, T$. Each state has an associated reward $R\left(S_{t}\right)$ which is a deterministic function of the state. ${ }^{1}$ A sequence of states has a utility which is a deterministic function of all these states $U\left(S_{1}, \ldots, S_{T}\right)$. Methods such as reinforcement learning (RL) uses observed rewards which are a function of the MDP state, to learn an optimal action sequence (policy), and is mainstay of modern AI in, for instance robotics and fine-tuning of large language models.

[^0]![](https://cdn.mathpix.com/cropped/2024_07_30_b6faca892de9afbc5a1cg-1.jpg?height=358&width=895&top_left_y=1974&top_left_x=592)

Figure 18.1.: Markov decision processes (MDP) are probabilsitic graphical models in which a sequence of actions $A_{t}$ influences a sequence of observed states $S_{t}$ in the world, which themselves also depend upon previous states.

![](https://cdn.mathpix.com/cropped/2024_07_30_b6faca892de9afbc5a1cg-2.jpg?height=412&width=972&top_left_y=325&top_left_x=542)

Figure 18.2.: The Kalman filter is a probabilistic model which captures dependence in time for a continuous state variable $Z_{t}$, upon which noisy observations of the state $X_{t}$ depend. This has identical probabilistic dependence structure to the HMM.

 18.2. Kalman filter

Similar to HMMs but for situations with continuous hidden state, the Kalman filter is often used for target tracking or motion smoothing with noisy observations. This has a wide array of applications in, for instance, automated airplane (autopilot) or ship guidance. For the linear-Gaussian case $^{2}$, the Kalman filter can be conceptualized as a predictor-corrector process to estimate the hidden state, which alternates between the two steps:

1. Using a model, predict the next state from the current estimate of the state,
2. When the next noisy observation of the state becomes available, correct the last prediction using a suitably weighted average of the noisy observation and the previous prediction.

It turns out that the weighted average arises directly from the use of Bayes' theorem, which optimally balances the uncertainty in the predictions, against the uncertainty in the noisy observations.

The Kalman filter captures dependence in time which is not directly observed, each continuous hidden state $Z_{t}$ depends only upon the one before it in time, $Z_{t-1}$ for all $t=1,2, \ldots, T$. The noisy observations $X_{t}$ depend only upon the associated hidden state, $Z_{t}$ (Figure 18.2). The simplest (and most widely encountered) model is a linear-Gaussian recurrence relation:

$$
\begin{align*}
Z_{t} & =A Z_{t-1}+U \\
X_{t} & =B Z_{t}+V \tag{18.1}
\end{align*}
$$

for all $t=1,2, \ldots, T$ and where matrices $A, B$ are state update and observation models, and $U$, $V$ are Gaussian error/noise vectors.

 18.3. Recurrent neural networks (RNN)

'Standard' deep nets discussed in section 13, cannot explicitly take data ordering into account. Recurrent neural networks (RNN) treat the hidden layer in the previous stage as an additional input to current stage in the ordering. The output of the previous stage's hidden layer $z_{t-1}$ is included with computation of the current hidden layer, $z_{t}$. In this way, order dependence is explicitly taken into account in the network structure (Figure 18.3). This defines a (nonlinear) recurrence relation,

\footnotetext{
${ }^{2}$ Models which are linear functions of the random variables, which are all (multivariate) Gaussian.

![](https://cdn.mathpix.com/cropped/2024_07_30_b6faca892de9afbc5a1cg-3.jpg?height=1009&width=895&top_left_y=312&top_left_x=586)

Figure 18.3.: Recurrent neural networks (RNNs) are deep networks which take into account time ordering of input data $x_{t}$ by treating the output of the hidden layer in stage $t-1$ as an additional input to the the next stage's network.

$$
\begin{align*}
& z_{t}=\max \left(0, W^{T} x_{t}+u^{T} z_{t-1}\right)  \tag{18.2}\\
& y_{t}=\max \left(0, v^{T} z_{t}\right)
\end{align*}
$$

for $t=1,2, \ldots, T$ with the additional state $z_{0}$ required to get the recursion started.

RNNs are widely used in modern AI applications such as seq2seq networks which map a source sequence into a target sequence, taking into account the sequential dependency in the inputs and outputs. Examples of applications where such networks are used include neural machine translation of text in one language to another language, and forms the basis of some chatbots and large language models.

 Learning outcomes for section 18

$\triangleright$ Define Markov decision processes, Kalman filters and recurrent neural networks: explain their significance for applications.

$\triangleright$ Recognize and provide the mathematical structure of these sequential models.


[^0]:    ${ }^{1}$ Deterministic means not random (although in this case, since the state itself is random, the reward as a function of the state is random).


 Terminology 


Activation function: In deep learning, each neuron has a nonlinear activation function acting on the accumulated inputs to create its output.
AI (artificial intelligence): Computational algorithms for solving complex, real-world problems. 
Algorithm strategy: A systematic principle for solving a broad class of $\mathrm{AI} / \mathrm{ML}$ problems. 
Annealing schedule: In the simulated annealing method for combinatorial optimization, the annealing schedule parameter determines for how long bad steps which worsen the objective, will be taken. 
Approximate method/algorithm: A computational algorithm which is only guaranteed to find a locally optimal value; this could also be globally optimal, but we cannot be sure. 
Approximate optimization algorithm: An approximate algorithm finds a 'good enough' solution to an optimization problem, not one which is guaranteed to be globally optimal. It may be locally optimal. 
Atomic statements: A primitive logical statement which has Boolean truth value, including the constants True and False. 
Automatic differentiation (AD): Gradients calculated exactly at a specific numerical value of the function, by breaking the function down into elementary expressions which have simple chain rule forms. 
Backpropagation: Gradients of objective functions of deep neural networks can be computed by backpropagating gradients through layers using the chain rule of calculus. 
Bayes' theorem: Given a conditional distribution and marginals, Bayes' theorem swaps the conditional. 
Bellman recursion: A recursive mathematical equation calculating the optimal configuration at each stage of a dynamic programming algorithm, from optimal configurations in the previous stage. 
Bellman's principle of optimality: Optimal solutions to optimization problems are composed of optimal solutions to sub-problems. 
Big-oh notation: A function of a single parameter representing the asymptotic behaviour of all members of its complexity class.
Boolean function: Function which take Boolean inputs, producing a Boolean output. The special logical operations “and”, “or”, “not”, “implies” and “if and only if” are called connectives. 
Boolean set: The set of values with two elements False, True which are used in logical computations. 
Chain rule of calculus: The gradient of a function of a function is obtained using this mathematical rule. 
Chain rule of probability: The distribution over every set of joint random variables can be expressed as an appropriate product of their conditionals. 
Child: In a directed graph, if there is an edge $X$ to $Y$, then $Y$ is a child of $X$. 
Classifier (ML): An algorithm, optimized on training data, which takes feature data as input and produces a classification decision as output. 
Cluster centroid: A cluster centroid is a representative value in the space of the data around which a cluster is defined. 
Clustering: A clustering of a data set is a partition of the data items into classes (clusters) such that some objective function which measures the quality of the clustering, is optimized. 
Combinatorial explosion: The size of the space of all possible configurations, grows rapidly with the size of the problem. 
Combinatorial neighbourhood: A set of configurations which are “close” to a given configuration, under some measure of combinatorial distance between configurations. 
Complex (logical) statements: A logical statement constructed from atomic logical statements using logical connectives. 
Complexity class: A measure of how rapidly the value of a function grows with the value of its input.
Computational complexity: The complexity class of the worst case number of computational steps an algorithm requires to complete.
Computational graph: A graphical representation of the computational operation of an algorithm.
Conditional distribution: Distribution arising from fixing the values of a subset of a collection of joint random variables. 
Conditional independence: In a PGM, if the parents of a variable $V$ are fixed and $U$ is not a parent of $V$, then $V$ is said to be conditionally independent of $U$. 
Configuration: A specific state of an AI or ML problem. 
Constant: A term in an algebraic expression with a fixed (semantic) value. 
Continuous event: In a probabilistic model, a continuous event is a specific outcome in an uncountably infinite set, for example, the real line. 
Convergence guarantee: An optimization algorithm which, when allowed to search for an infinite number of steps, is guaranteed to find the global optima, is said to have guaranteed convergence. 
Convex optimization algorithm: A convex function has one minima which is the global one. 
Convolutional neural network (CNN): A deep neural network with weight sharing over ‘sliding windows’ of the input connections to each layer. 
Decision (classification) boundary: In classification in ML, a set of (one or more) boundaries which partition the data into two or more classes. Linear classifiers have (in general) hyperplane boundaries.
Directed acyclic graph (DAG): Nodes with directed edges between them in which there are no directed cycles, that is, no directed paths which lead from a node back to itself.
Discrete outcome: In a probabilistic model, a discrete event is a specific item in a (countable) set. 
Divergent iteration: In solving an optimization problem, if the algorithm does not converge on a solution, it can instead diverge so that the algorithm never terminates. 
Dual numbers: An algebraic tool for efficiently computing the gradient of composite functions using the chain rule of calculus. 
Dynamic programming (DP): An efficient algorithm strategy for solving combinatorial AI problems in polynomial time, using factorization and the principle of optimality.
Edge: A directed graph consists of points (nodes) which are joined by lines with arrowheads (edges). 
Error function (ML): In machine learning, an error function measures the extent to which an algorithm can predict the data. 
Evidence distribution: In Bayes’ theorem, the evidence distribution is the marginal of the likelihood variable. 
Exact method/algorithm: A computational algorithm which is guaranteed to find the globally optimal value of an objective function. 
Exhaustive method: Solving a problem by testing all possible configurations.
Factorization: A factorized expression uses distributivity to create bracketed expressions, for example $a × b + a × c$ can be factorized as $a × (b + c)$. 
Function composition: Feeding the output of one function into the input of another, and so on. 
Globally optimal value: No other value of the objective function is better than the globally optimal value.
Greedy approximate search: Also known as hill-climbing or iterative improvement, an algorithm which uses successive neighbourhood search to find a local optima. 
Hidden Markov model (HMM): Markov chain where the sequence of time-ordered random variables is not directly measured and must be inferred from the observations. 
Independent random variables: If the joint distribution of random variables can be factored into a product of their marginal distributions, we say that the variables are independent. 
Indicator function: A function which returns the value 1 if a given logical condition is true, and 0 otherwise. 
Joint random variables: Two or more random events co-occurring. Their combined distribution is called the joint distribution. 
K-means algorithm: An iterative method for approximately solving the K-means clustering problem. 
K-means error (objective): A least-sum-of-squares objective measuring the quality of a clustering partition; also the name for an iterative algorithm which minimizes this objective. 
Kalman filter: A linear-Gaussian sequential model with continuous hidden state and noisy observations, which optimally balances prediction error against observation error. 
Learning rate (ML): The size of each step in a gradient descent algorithm for optimization of machine learning model parameters. 
Likelihood distribution: In Bayes’ theorem, the conditional distribution to be reversed to obtain the posterior distribution. 
Linear classifier (ML): Classifier with hyperplane decision boundary. 
Linear regression (ML): Curve-fitting a linear function (hyperplane) of continuous parameters. 
Linearly separable data: Data which can be perfectly separated into two classes by a linear classifier which has a hyperplane decision boundary. 
Locally optimal value: In some part of (some subset) of the configuration space, no other value of the objective function is better than the locally optimal value in that subset. 